<<<<<<< HEAD
{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CNN_solutions.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3-final"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"p_5PWr7JoIbf"},"source":["# IVADO-Mila Deep Learning School\n","# Fall 2019\n","# Tutorial : Convolution neural networks (CNNs)\n","\n","## Authors: \n","\n","Margaux Luck <margaux.luck@rd.mila.quebec>\n","\n","Jeremy Pinto <jeremy.pinto@rd.mila.quebec>\n","\n","Mathieu Germain <mathieu.germain@rd.mila.quebec>\n","\n","### Translation to English: \n","\n","Laurent Charlin <lcharlin@gmail.com>\n","\n","## Intro\n","\n","This tutorial uses concrete examples to introduce the fundamental concepts behind convolutional neural networks."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"A73O9J3necuk"},"source":["# Initialization"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0PNXwEm-ej9a"},"source":["\n","Before we begin, you must ensure to install all required libraries for this tutorial. To do so, we will use the `pip` utility. Simply execute the cell below by selecting it and pressing `shift`+`Enter`. (This operation may take a few minutes.)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"fNlYWG6z9GKT","scrolled":true},"outputs":[],"source":["!pip3 install torch torchvision Pillow matplotlib"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ddGNnGb0lMFJ"},"source":["To ensure that all required libraries are available, let's try to load all libraries and modules we will need during this tutorial by executing this cell: "]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"pillow unavailable\n"}],"source":["#import imp.find_module('torch')\n","import importlib\n","required_libraries = ['torch', 'torchvision', 'pillow', 'matplotlib']\n","for lib in required_libraries:\n","    if importlib.util.find_spec(lib) is None:\n","        print(\"%s unavailable\" % lib)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Torch version:  1.3.1\nGPU Available: False\n"}],"source":["import torch\n","\n","use_gpu = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n","\n","print(\"Torch version: \", torch.__version__)\n","print(\"GPU Available: {}\".format(use_gpu))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FNomSpLO9LeH"},"source":["# The MNIST dataset\n","MNIST is the classic go-to **classification dataset** used in **computer vision**. It is available here: <a href=\"http://yann.lecun.com/exdb/mnist/\">Yann LeCun's website</a>. \n","\n","Each datum is an **image of a handwritten digit**. Here are a few examples from this dataset: \n","\n","![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/mnist.png?raw=true)\n","\n","Each image also comes with a **class label** which indicates which digit does the image correspond to. For example, the labels of the examples above are 5, 0, 4, and 1 respectively. The classes are balanced which means that all digits appear (roughly) the same number of times in the dataset.\n","\n","The dataset is composed of **60 000 training examples** and **10 000 test examples**. All images have exactly the same size (**28x28 pixels** or 28 rows by 28 columns). Each pixel is represented by a number between 0 and 255 which represents its grey level (0 is white and 255 is black). Depending on the model, each image may have to be flattened (to a 784-length vector)."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wk9AMaT9hmcX"},"source":["## Objective\n","\n","Our goal is to find a model that correctly classifies these images. In particular, our model will take as input (features/covariates/independant-variables) an image and will predict its digit (label/dependant variable). This problem can be formalized as follows:\n","\n","`f(image) = predicted digit`\n","\n","where `f` is a function.\n","\n","In this tutorial, we will consider both **multi-layer perceptrons (MLPs)** and **convolutional neural networks** as functions for solving this prediction problem. Both models take as inputs pixel intensities which will be modified using mathematical operations through the layers of the networks. Their output is a vector of size *1x10* where each entry corresponds to the (normalized) score that the input image is a particular digit. The sum over these 10 values is 1, and each score is non-negative. This is why these scores can be interpreted as probabilities. Our final prediction will be the entry with the highest score. For example, this prediction\n","\n","`[0.8, 0.1, 0, 0, 0, 0.05, 0.05, 0.0, 0.0, 0.0, 0.0]`\n","\n","indicates that the model assigns a score of 0.8 to class 0.\n","\n","Learning implies finding the parameters of a model that will maximize the model's performance. To learn, we will start by randomly initializing the parameters of our model. Then we iterate through examples. For each example we will obtain the network's prediction, compare it with the true label, and then update the parameters of the models to obtain a better prediction. We do this until we reach some predetermined stopping criteria."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"F1U2sDz9ufy9"},"source":["## Download the dataset and create the data loader\n","\n","Before we begin training, we have to obtain the MNIST dataset. It turns out that there are built-in functions within PyTorch to do so.\n","\n","### Tool box\n","**Note:** PyTorch comes with function to load, shuffle, and augment data.\n","\n","Here is an easy way to load the data in PyTorch: \n","<ol>\n","<li>Subclass <a href=\"http://pytorch.org/docs/master/data.html#torch.utils.data.Dataset\">`torch.utils.data.Dataset`</a> and add  `__getitem__` and `__len__` methods.</li>\n","<li>Then you can use<a href=\"http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader\">`torch.utils.data.DataLoader`</a> to read and load the data into memory.</li>\n","</ol>\n","\n","It is even easier for MNIST in PyTorch since there is already a subclass of \"datasets\" defined for it: <a href=\"http://pytorch.org/docs/master/torchvision/datasets.html#mnist\">`torchvision.datasets.MNIST`</a>.\n","\n","<a href=\"http://pytorch.org/docs/master/torchvision/datasets.html\">Other datasets are also similarly available</a>\n","\n","**Note:** <a href=\"http://pytorch.org/docs/master/tensors.html#torch.Tensor.view\">`torch.Tensor.view()`</a> returns a new tensor with the same data as the original tensor but a different shape. For example, it can be used to flatten an image."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"0it [00:00, ?it/s]Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n 99%|█████████▉| 9838592/9912422 [00:26<00:00, 367986.91it/s]Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n\n0it [00:00, ?it/s]\u001b[ADownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/28881 [00:00<?, ?it/s]\u001b[A\n 57%|█████▋    | 16384/28881 [00:01<00:00, 63602.76it/s]\u001b[A\n32768it [00:01, 30411.07it/s]\u001b[A\n0it [00:00, ?it/s]\u001b[AExtracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[A\n  1%|          | 16384/1648877 [00:00<00:25, 64602.25it/s]\u001b[A\n  2%|▏         | 40960/1648877 [00:00<00:22, 72970.57it/s]\u001b[A\n  5%|▍         | 81920/1648877 [00:01<00:18, 86635.85it/s]\u001b[A\n 10%|▉         | 163840/1648877 [00:01<00:13, 111875.88it/s]\u001b[A\n9920512it [00:30, 330414.85it/s]\nException in thread Thread-4:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n    self.run()\n  File \"/home/andria/.local/lib/python3.7/site-packages/tqdm/_tqdm.py\", line 148, in run\n    for instance in self.tqdm_cls._instances:\n  File \"/usr/lib/python3.7/_weakrefset.py\", line 60, in __iter__\n    for itemref in self.data:\nRuntimeError: Set changed size during iteration\n\n1654784it [00:06, 255090.75it/s]\n0it [00:00, ?it/s]Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n8192it [00:00, 15942.06it/s]\nExtracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\nProcessing...\nDone!\n# of train examples: 48000\n# of valid examples: 12000\n# of test examples: 10000\n"}],"source":["import numpy as np\n","import random\n","import torch\n","from torch.utils.data import sampler, DataLoader\n","from torchvision.datasets import MNIST\n","import torchvision.transforms as transforms\n","\n","\n","manualSeed = 1234\n","use_gpu = torch.cuda.is_available()\n","\n","# Fixing random seed\n","random.seed(manualSeed)\n","np.random.seed(manualSeed)\n","torch.manual_seed(manualSeed)\n","if use_gpu:\n","   torch.cuda.manual_seed_all(manualSeed)\n","\n","class ChunkSampler(sampler.Sampler):\n","    \"\"\"Samples elements sequentially from some offset.\n","    From: https://github.com/pytorch/vision/issues/168\n","    \n","    Parameters\n","    ----------\n","    num_samples: int\n","      # of desired datapoints\n","    start: int\n","      Offset where we should start selecting from\n","    \"\"\"\n","    def __init__(self, num_samples, start=0):\n","        self.num_samples = num_samples\n","        self.start = start\n","\n","    def __iter__(self):\n","        return iter(range(self.start, self.start + self.num_samples))\n","\n","    def __len__(self):\n","        return self.num_samples\n","\n","\n","train_dataset = MNIST(root='../data', \n","                      train=True, \n","                      transform=transforms.ToTensor(),  \n","                      download=True)\n","\n","test_dataset = MNIST(root='../data', \n","                     train=False, \n","                     transform=transforms.ToTensor())\n","\n","train_dataset_sizes = len(train_dataset)\n","num_train_samples = int(0.8 * train_dataset_sizes)\n","num_valid_samples = train_dataset_sizes - num_train_samples\n","num_test_samples = len(test_dataset)\n","\n","print('# of train examples: {}'.format(num_train_samples))\n","print('# of valid examples: {}'.format(num_valid_samples))\n","print('# of test examples: {}'.format(num_test_samples))\n","\n","batch_size = 100\n","\n","train_loader = DataLoader(dataset=train_dataset,\n","                          sampler=ChunkSampler(num_train_samples, 0),\n","                          batch_size=batch_size, \n","                          shuffle=False)\n","\n","valid_loader = DataLoader(dataset=train_dataset,\n","                          sampler=ChunkSampler(\n","                              num_valid_samples, num_train_samples),\n","                          batch_size=batch_size, \n","                          shuffle=False)\n","\n","test_loader = DataLoader(dataset=test_dataset, \n","                         batch_size=batch_size, \n","                         shuffle=False)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gRV8zZbHV6zN"},"source":["Let's visualize the training data!"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Inputs size: torch.Size([100, 1, 28, 28])\nClasses size: torch.Size([100])\n\n\nDisplay the first image:\n"},{"data":{"image/png":"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\n","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 251.565 248.518125 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \nL 244.365 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pe53a0d0079)\">\n    <image height=\"218\" id=\"imagea0a59a64e4\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAABHNCSVQICAgIfAhkiAAABBdJREFUeJzt3VFu2kAUQFGosi+SlTlZWZKV0QU09bSMfA34nF+LYhquRuJpPOfr9Xo9AZv6tfcNwBEIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CL3vfwF6+vr5Wr7+9vTU3coPX19fV65+fn82N8M+saBAQGgSEBgGhQUBoEBAaBIQGgfP1er3ufRN7OJ/Pe9/CZkZztsvlcvNrR9f5mRUNAkKDgNAgIDQICA0CQoPA0/68v/U2mGVZpl4/4+PjY7f3Hhlt0TnqeMCKBgGhQUBoEBAaBIQGAaFBQGgQeNo52vv7++r12VnUnv9ts1t81maA39/fq68dzSdn3nv0N3tkVjQICA0CQoOA0CAgNAgIDQJCg8Bhj226Z7PzpC33hM3u85uZXz7ynM2KBgGhQUBoEBAaBIQGAaFBQGgQeNr9aCOze7q2nFXN7qUbvffo3re0Nmcbzege+atqRYOA0CAgNAgIDQJCg4DQIHDYn/fv+Sf0rY+c2vNPvvbZRp/rkY+EsqJBQGgQEBoEhAYBoUFAaBAQGgQO+7i50cxlNEcbzbrWro/ee/b6zBxuzy00I6PPZY4GByc0CAgNAkKDgNAgIDQICA0C5mh/sSzL6vWZOdvsvOdyudz83qPrs3vCRkb39qysaBAQGgSEBgGhQUBoEBAaBIQGgcM+13HWzLFPWz+fcPaZlTNm548z//boc+/JigYBoUFAaBAQGgSEBgGhQUBoEDBHu9Fo39bMfrStn624Nm/acsY265G/qlY0CAgNAkKDgNAgIDQICA0Cft6/0czRSCNbb6NZM/Ooun+xdu/3fOzSLCsaBIQGAaFBQGgQEBoEhAYBoUHAHG0jW25F2XPOxm2saBAQGgSEBgGhQUBoEBAaBIQGAXO0Hcw8qu502v9xdfw/KxoEhAYBoUFAaBAQGgSEBgGhQcAc7Q6dz+ep15uz3R8rGgSEBgGhQUBoEBAaBIQGAaFB4GXvG+BPoznX6LmQM2eceSbkNqxoEBAaBIQGAaFBQGgQEBoEbJN5QGtHQp1Oc8dC+Tpsw4oGAaFBQGgQEBoEhAYBoUFAaBAwR3tAo20wo2Oh1izLsnp9NMPjZ1Y0CAgNAkKDgNAgIDQICA0CQoOAx809oNljmWbmbNzGigYBoUFAaBAQGgSEBgGhQUBoEDBHe0KOXro/VjQICA0CQoOA0CAgNAgIDQJ+3j8gTxjsWdEgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CvwHHlvf/EYA2DQAAAABJRU5ErkJggg==\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m9afce59c68\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m9afce59c68\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m9afce59c68\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m9afce59c68\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m9afce59c68\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m9afce59c68\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m9afce59c68\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m508f5c6011\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m508f5c6011\" y=\"11.082857\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m508f5c6011\" y=\"49.911429\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m508f5c6011\" y=\"88.74\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m508f5c6011\" y=\"127.568571\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m508f5c6011\" y=\"166.397143\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m508f5c6011\" y=\"205.225714\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 224.64 \nL 26.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 224.64 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe53a0d0079\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","inputs, classes = next(iter(train_loader))\n","\n","print('Inputs size: {}'.format(inputs.size()))\n","print('Classes size: {}'.format(classes.size()))\n","\n","# Random image of the batch\n","img1 = 255 - inputs[np.random.randint(len(inputs))] * 255\n","\n","# Plot the image\n","print('\\n\\nDisplay the first image:')\n","img1 = img1.numpy()[0, :, :]\n","plt.imshow(img1, cmap='gray', vmin=0, vmax=255)\n","plt.grid(False)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"T_XSUTbG0UvX"},"source":["# CPU or GPU\n","**Note:** <a href=\"http://pytorch.org/docs/master/cuda.html#module-torch.cuda\">`torch.cuda`</a> is a library which can perform tensor operations using GPUs. Specifically, the library includes CUDA tensors which offer the same operations as regular tensors but instead run on GPUs, instead of CPUs.\n","<a href=\"http://pytorch.org/docs/master/cuda.html#torch.cuda.is_available\">`torch.cuda.is_available()`</a> returns whether or not CUDA is available. Adding `.to(\"cuda:0\")` to the variable identifying a CPU tensor, returns the equivalent GPU tensor.\n","\n","For more information about using GPUs on colab, please refer to this [tutorial](https://colab.research.google.com/drive/1y3ZE4m-D7lPoMzsypSEXessYmjWfKGqD#scrollTo=3IEVK-KFxi5Z).\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"GPU Available: False\n"}],"source":["use_gpu = torch.cuda.is_available()\n","\n","print(\"GPU Available: {}\".format(use_gpu))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xjWUALQGXQbi"},"source":["We are going to compare the performance of a CNN to the performance of a vanilla MLP. Below is a quick reminder of MLPs, the architecture of the MLP we will be using, as well as the code to train the MLP. \n","\n","# Multi-layer perceptron (MLP)\n","A multi-layer perceptron is a vanilla few-forward neural network. Our instanciation will take as input an image, will transform it through a series of hidden layers and then will pass it to an output layer. This output is a vector of 10 numbers where each represents the normalized score of a particular class (this is sometimes interpreted as a probability).\n","\n","For example, here an MLP architecture to classify MNIST images: \n","![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/mlp.png?raw=true)\n","\n","Whenever you are trying to solve a prediction task, the process usually goes as follows: \n","<ol>\n","<li>Determine the network's artchitecture. This will implicitely determine the number of parameters (weights and biases) of the network.</li>\n","<li>Determine the cost function and the optimization method.</li>\n","<li>Train the weights of the network (i.e., fit the model to train data).</li>\n","<li>Test the network (i.e., evaluate its performance on test data).</li>\n","</ol>\n","\n","This procedure is general and applies to all types of (deep) neural networks.\n","\n","### Toolbox\n","\n","Racall that a (deep) neural network can be coded by using the library <a href=\"http://pytorch.org/docs/master/nn.html\">`torch.nn`</a>. `nn` uses <a href=\"http://pytorch.org/docs/master/autograd.html\">`torch.autograd`</a> to instantiate and computer the gradients (of the loss function with respect to the parameters)."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"MLP(\n  (hidden_layer): Sequential(\n    (0): Linear(in_features=784, out_features=500, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=500, out_features=500, bias=True)\n    (3): ReLU()\n  )\n  (output_layer): Sequential(\n    (0): Linear(in_features=500, out_features=10, bias=True)\n  )\n)\n\n\n# Parameters:  648010\n"}],"source":["import torch.nn as nn\n","import copy\n","\n","input_size = 784\n","hidden_size = 500\n","num_classes = 10\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(MLP, self).__init__()\n","\n","        self.hidden_layer = nn.Sequential(\n","            nn.Linear(input_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU())\n","        \n","        self.output_layer = nn.Sequential(\n","            nn.Linear(hidden_size, num_classes))\n","    \n","    def forward(self, x):        \n","        \n","        out = self.hidden_layer(x)\n","        \n","        out = self.output_layer(out)\n","        \n","        return out\n","\n","model = MLP(input_size, hidden_size, num_classes)\n","# switch model to GPU\n","model = model.to(device)\n","\n","print(model)\n","\n","print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))\n","\n","# Save the initial weights of model\n","init_model_wts = copy.deepcopy(model.state_dict())\n","\n","learning_rate = 1e-2\n","\n","criterion = nn.CrossEntropyLoss()  \n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"Bdh8ocHBXQbn"},"outputs":[],"source":["import time\n","from torch.autograd import Variable\n","\n","model.load_state_dict(init_model_wts)\n","\n","since = time.time()\n","\n","num_epochs = 10\n","train_loss_history = []\n","valid_loss_history = []\n","\n","print(\"# Start training #\")\n","for epoch in range(num_epochs):\n","    \n","    train_loss = 0\n","    train_n_iter = 0\n","    \n","    # Set model to train mode\n","    model.train()\n","    \n","    # Iterate over train data\n","    for images, labels in train_loader:  \n","        \n","        # put images on proper device (GPU)\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        \n","        # Flatten the images\n","        images = images.view(-1, 28*28)\n","\n","        # Zero the gradient buffer\n","        optimizer.zero_grad()  \n","        \n","        # Forward\n","        outputs = model(images)\n","        \n","        loss = criterion(outputs, labels)\n","        \n","        # Backward\n","        loss.backward()\n","        \n","        # Optimize\n","        optimizer.step()\n","        \n","        # Statistics\n","        train_loss += loss.item()\n","        train_n_iter += 1\n","    \n","    valid_loss = 0\n","    valid_n_iter = 0\n","    \n","    # Set model to evaluate mode\n","    model.eval()\n","    \n","    # Iterate over valid data\n","    for images, labels in valid_loader:  \n","        \n","        # put images on proper device (GPU)\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        \n","        # Flatten the images\n","        images = images.view(-1, 28*28)\n","        \n","        \n","        # Forward\n","        outputs = model(images)\n","        \n","        loss = criterion(outputs, labels)\n","        \n","        # Statistics\n","        valid_loss += loss.item()\n","        valid_n_iter += 1\n","    \n","    train_loss_history.append(train_loss / train_n_iter)\n","    valid_loss_history.append(valid_loss / valid_n_iter)\n","    \n","    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n","    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n","    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n","\n","time_elapsed = time.time() - since\n","\n","print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n","    time_elapsed // 60, time_elapsed % 60))"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"UhO4qn_vXQbq"},"outputs":[],"source":["# Save history for later\n","mlp_train_loss_history = train_loss_history\n","mlp_valid_loss_history = valid_loss_history\n","\n","# Plot training and validation curve\n","x = range(1, num_epochs + 1)\n","plt.plot(x, mlp_train_loss_history, label='train')\n","plt.plot(x, mlp_valid_loss_history, label='valid')\n","\n","plt.xlabel('# epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"3W5NEuERXQbt"},"outputs":[],"source":["# Set model to evaluate mode\n","model.eval()\n","\n","correct = 0\n","total = 0\n","\n","# Iterate over test data\n","for images, labels in test_loader:\n","    \n","    # put images on proper device (GPU)\n","    images = images.to(device)\n","    labels = labels.to(device)\n","    \n","    # Flatten the images\n","    images = images.view(-1, 28*28)\n","    \n","\n","    # Forward\n","    outputs = model(images)\n","    _, predicted = torch.max(outputs.data, 1)\n","    \n","    # Statistics\n","    total += labels.size(0)\n","    correct += torch.sum(predicted == labels.data)\n","\n","print('Accuracy on the test set: {:.2f}%'.format(100 * correct / total))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Hp1YgKZb8btG"},"source":["# Convolutional neural networks (CNNs)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GMbU21dg3Rqx"},"source":["## Concepts\n","\n","We first review the basic concepts that underlie CNNs.\n","\n","### Convolution\n","\n","A convolution \"slides\" a filter *K* along image *I* to obtain an output *I*\\**K*.\n","\n","Here is an example of a 2D convolution:\n","\n","![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/numerical_no_padding_no_strides.gif?raw=true)\n","\n","\n","### Filters\n","\n","Filters (or kernels) are used to extract information useful to the task from their input. Filters are generally of size *n* \\* *n* where *n* is usually odd. The filters are parametrized by weights, one for each of its entry, which are learned by the convolutional network.\n","\n","The filter used in the previous example is:\n","\n","![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/d7acc4aeb74d9e9cb5fb51482a302196594837fe.png?raw=true)\n","\n","### Depth\n","\n","We typically use *M* of filters which can be understood as the depth of the layer (see below). Note that this is different from the depth of the network (which is the number of layers). M is a hyperparameter. Here, each filter's output (blue circles) is represented as a single depth dimension on the output.\n","\n","![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/Conv_layer.png?raw=true)\n","\n","### Stride\n","\n","The stride corresponds to the number of pixels the filter moves over in between each step of the convolution. The stride is measured in terms of a number of pixels. We typically use strides of 1 or 2. The larger the stride, the smaller the dimension of the output.\n","\n","### Zero padding\n","\n","Zero padding consists in padding (adding) a border of zeros around the input image. This can be useful to preserve the dimension from input to output.\n","\n","Below is an example of a zero padding which preserves the dimensions from input to output. Here, zero padding is set to 1, stride is set to 1, and the filter has size 3x3.\n","\n","![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/same_padding_no_strides.gif?raw=true)\n","\n","\n","### Max Pooling\n","\n","In addition to convolutions, CNNs usually have pooling layers. The goal of pooling is to reduce the dimensionality of the input in-between two convolution layers to reduce the number of parameters in the network. For example, the famous LeNet CNN, uses max pooling with 2x2 filters and a stride of 2. Max pooling outputs the max value in a 2x2 region. This output is then the input of the next layer.\n","\n","Here is an example of the max pooling operation:\n","\n","![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/maxpool.jpeg?raw=true)\n","\n","\n","### Receptive Field\n","\n","The receptive field is a measure of the CNNs' capacity to perceive information at different input scales. In an MLP (fully connected), the features are connected to every neuron. The receptive field of this MLP is the full image.\n","\n","For CNNs, convolution operations typically imply sparse connections. In other words, each filter only has a local receptive field. However, each successive layer will have access to a slightly larger receptive field. \n","\n","Let's look at an example. Imagine a 3x3 filter with stride set to 1. In this case, the first layer's receptive field is a maximum of 3x3. However, the more layer we add the more we increase the network's receptive field. Adding a second layer with 3xe3 filters and a stride of 1, our receptive field is increased to 5x5. Adding a third 3x3 layer further increases our receptive field to 7x7.\n","\n","What is the advantage of using multiple smaller successive filters instead of a single larger one? A single large filter of 7x7 implies 49 parameters. Instead 3 layers of 3x3 filters requires only 27 parameters (9 \\* 3). It is therefore more efficient to use multiple successive filters and in both cases the receptive field is the same (7x7). In addition, by using multiple successive filters, we can introduce more non-linearities in the model (one after each filter).\n","\n","Here the 3x3 filter (in grey) with a stride of 1 has a receptive field of 5x5 (yellow region):\n","\n","![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/sNBmKMKAz-yJeCuS14usSqw.png?raw=true)\n","\n","\n","### Number of dimensions\n","\n","In general, for a convolutional layers with input dimensions (volume)\n","$W_1 * H_1 * D_1$ and hyperparameters \n","\n","* Number of filters = $K$\n","\n","* Filter sizes = $F$\n","\n","* Stride = $S$\n","\n","* Zero Padding = $P$.\n","\n","We obtain an output volume of $W_2 * H_2 * D_2$ dimensions where  \n","\n","* $W_2 = (W_1 - F + 2P) / S + 1$\n","* $H_2 = (H1 - F + 2P) / S + 1$\n","* $D_2 = K$\n","\n","and the total number of parameters is $(F⋅F⋅D_1)⋅K$ weights and $K$ biases.\n","\n","For an in-depth analysis see [this paper](http://cs231n.github.io/convolutional-networks/)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"owIcd9VguyAe"},"source":["\n","## LeNet\n","CNNs have been developed to model images. They can model images more efficiently (with fewer parameters) than an equivalent MLP. LeNet is a basic CNN for classification. It comes in several versions.\n","\n","We will use a \"LeNet 5\" to classify MNIST digit images:\n","\n","![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/lenet5.png?raw=true)\n","\n","\n","To solve a prediction task, the process usually goes as follows (it is the same for MLPs and CNNs):\n","<ol>\n","<li>Determine the network's architecture. This will implicitly determine the number of parameters (weights and biases) of the network.</li>\n","<li>Determine the cost function and the optimization method.</li>\n","<li>Train the weights of the network (i.e., fit the model to train data).</li>\n","<li>Test the network (i.e., evaluate its performance on test data).</li>\n","</ol>\n","\n","\n","## Determining the network's architecture\n","### Toolbox\n","**Recall:** To instantiate a particular network in PyTorch, one first subclasses <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Module\">`torch.nn.Module`</a> and then writes the following methods :\n","<ul>\n","<li>The `__init__` method defines the layers. </li>\n","<li>The `forward(input)` method returns the `output`.</li>\n","</ul>\n","\n","\n","For LeNet 5's '`__init__`, the following classes can be used:\n","<ul>\n","<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Conv2d\">`torch.nn.Conv2d(in_channels, out_channels, kernel_size)`</a> applies a 2D convolution on the input channels.</li>\n","<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.MaxPool2d\">`torch.nn.MaxPool2d(kernel_size)`</a> applies 2D max pooling on the input channels.</li>\n","<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Linear\">`torch.nn.Linear(in_features, out_features)`</a> applies a linear transformation on its input: y = Ax + b.</li>\n","<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.ReLU\">`torch.nn.Relu()`</a> applies an elementwise Relu activation: Relu(x) = max(0, x).</li>\n","<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Softmax\">`torch.nn.Softmax(dim)`</a> applies a softmax activation to an n-dimensional tensor (normalizes the exponentiated entries).</li>\n","<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Sequential\">`torch.nn.sequential`</a> a sequential container in which to add modules in the order in which they will be constructed.</li>\n","</ul>\n","\n","`forward(input)` successively applies the input data to the different layers defined in  `__init__`.\n","\n","Finally, `model.to(\"cuda:0\")` passes the model to an available GPU.\n","\n","### Implementation"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"LeNet5(\n  (block1): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc): Linear(in_features=1568, out_features=10, bias=True)\n)\n\n\n# Parameters:  28938\n"}],"source":["class LeNet5(nn.Module):\n","    def __init__(self):\n","        super(LeNet5, self).__init__()\n","        self.block1 = nn.Sequential(\n","            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2))\n","        \n","        self.block2 = nn.Sequential(\n","            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2))\n","        \n","        self.fc = nn.Linear(7*7*32, 10)\n","        \n","\n","    def forward(self, x):\n","        out = self.block1(x)\n","\n","        out = self.block2(out)\n","        \n","        # Flatten the output of block2\n","        out = out.view(out.size(0), -1)\n","        \n","        out = self.fc(out)\n","        \n","        return out\n","        \n","model = LeNet5()\n","model = model.to(device)\n","\n","print(model)\n","print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tRo03AR2PFPE"},"source":["We note that LeNet4 has 28 938 parameters versus 648 010 parameters for an somewhat equivalent MLP with two hidden layers. This reduction in the number of parameters is significant. \n","\n","Here is how we calculate the number of parameters for LeNet5: \n","\n","```\n","1st layer: 16 filters of size 5x5 + 16 biases = 16*5*5 + 16 = 416\n","2nd layer: 16 * 32 filters of size 5x5 + 32 biases = 16*32*5*5 + 32 = 12 832\n","FC layer: 7*7*32*10 + 10 biases = 15 690\n"," \n","Total = 416 + 12 832 + 15 690 = 28 938\n","```\n","\n","As a comparison, here is how we calculate the number of parameters of the two hidden layer MLP: \n","The input flattens the 28x28 images into a vector of size 784. The second layer has 500 neurons. Each neuron requires 784 weights + 1 bias. So 500\\*785 parameters. This is then fed to another layer of 500 neurons which adds 501\\*500 parameters. Finally, the output layer has 10 neurons, each with 500 weights and a single bias for a total of 10\\*501 parameters. \n","\n","So in total we have: \n","```\n","500*785 + 501*500 + 10*501 = 648010\n","``` parameters."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"WuPJFPoDQYLq"},"outputs":[],"source":["# Save the initial weights of model\n","init_model_wts = copy.deepcopy(model.state_dict())"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y9H0ssbh3V1j"},"source":["## Determine the cost function and the optimization method\n","### Toolbox\n","**Recall:** A common choice for a multi-class task are the following:\n","<ul>\n","<li>**Cost function :** <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss\">`torch.nn.CrossEntropyLoss()`</a>. The cross entropy is often used in this context. It compares a (multivariate) distribution $p$ with a reference distribution $t$. It is minimized for $p=t$ and it is expressed mathematically by: $-\\sum_j t_{ij} \\log(p_{ij})$ where $p$ is the prediction, $t$ the target, $i$ are examples and $j$ the target class.</li>\n","<li>**Optimization method :** <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD(net.parameters(), lr=learning_rate)`</a> a standard stochastic gradient descent (SGD) implementation</li>\n","</ul>\n","\n","### Implementation\n"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"uE3Cfrd-hJ0e"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sYEtUb4s3rab"},"source":["## Train the weights of a network\n","### Toolbox\n","**Recall:** Training a network usually involves iterating for multiple epochs over the training dataset. One epoch corresponds to one pass over the full dataset. \n","\n","The dataset is usually divided into batches. Each epoch will then receive sequentially batches. For each batch we do the following operations:\n","<ol>\n","<li>`optimizer.zero_grad()`: we clear the previously stored gradients.</li>\n","<li>`loss.backward()`: we evaluate the cost, the gradients, and backpropagate the gradients through the computation graph.</li>\n","<li>`optimizer.step()`: we update the parameters using the previously calculated gradients. For SGD, the update is: `weight = weight - learning_rate * gradient`.</li>\n","</ol>\n","\n","### Implementation\n","Just fill in the blanks... Good luck!\n"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"elxupovwhRSk"},"outputs":[],"source":["model.load_state_dict(init_model_wts)\n","\n","since = time.time()\n","\n","num_epochs = 10\n","train_loss_history = []\n","valid_loss_history = []\n","\n","print(\"# Start training #\")\n","for epoch in range(num_epochs):\n","    \n","    train_loss = 0\n","    train_n_iter = 0\n","    \n","    # Set model to train mode\n","    model.train()\n","    \n","    # Iterate over train data\n","    for images, labels in train_loader:  \n","\n","        # put images on proper device (GPU)\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        \n","        # Zero the gradient buffer\n","        optimizer.zero_grad()          \n","        \n","        # Forward pass\n","        outputs = model(images)\n","        \n","        loss = criterion(outputs, labels)\n","        \n","        # Backward\n","        loss.backward()\n","        \n","        # Optimize\n","        optimizer.step()\n","        \n","        # Statistics\n","        train_loss += loss.item()\n","        train_n_iter += 1\n","    \n","    valid_loss = 0\n","    valid_n_iter = 0\n","    \n","    # Set model to evaluate mode\n","    model.eval()\n","    \n","    # Iterate over valid data\n","    for images, labels in valid_loader:  \n","        \n","        # put images on proper device (GPU)\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        \n","        # Forward\n","        outputs = model(images)\n","        \n","        loss = criterion(outputs, labels)\n","       \n","        # Statistics\n","        valid_loss += loss.item()\n","        valid_n_iter += 1\n","    \n","    train_loss_history.append(train_loss / train_n_iter)\n","    valid_loss_history.append(valid_loss / valid_n_iter)\n","    \n","    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n","    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n","    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n","\n","time_elapsed = time.time() - since\n","\n","print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n","    time_elapsed // 60, time_elapsed % 60))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3fozzXmdRGTs"},"source":["Let's plot the training curves!"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"TUGbFeg5RHFZ"},"outputs":[],"source":["# Save history for later\n","lenet5_train_loss_history = train_loss_history\n","lenet5_valid_loss_history = valid_loss_history\n","\n","# Plot training and validation curve\n","x = range(1, num_epochs + 1)\n","plt.plot(x, lenet5_train_loss_history, label='train')\n","plt.plot(x, lenet5_valid_loss_history, label='valid')\n","\n","plt.xlabel('# epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"H6kWtULrSDXL"},"source":["We can overlay the validation curves on top of the training curves for training of LeNet5:"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"rE1qsmvaSTjH"},"outputs":[],"source":["# Plot training and validation curve\n","x = range(1, num_epochs + 1)\n","plt.plot(x, mlp_train_loss_history, label='MLP train')\n","plt.plot(x, mlp_valid_loss_history, label='MLP valid')\n","plt.plot(x, lenet5_train_loss_history, label='LeNet5 train')\n","plt.plot(x, lenet5_valid_loss_history, label='LeNet5 valid')\n","\n","plt.xlabel('# epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Tq79RTld3xyc"},"source":["## Test the network\n","### Toolbox\n","**Recall:** we evaluate the network's performance on test data.\n","### Implementation"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"da47-MilhpN7"},"outputs":[],"source":["# Set model to evaluate mode\n","model.eval()\n","\n","correct = 0\n","total = 0\n","\n","# Iterate over data.\n","for images, labels in test_loader:\n","    \n","    # put images on proper device (GPU)\n","    images = images.to(device)\n","    labels = labels.to(device)\n","    \n","    # No need to flatten the images here !\n","    \n","\n","    # Forward\n","    outputs = model(images)\n","    _, predicted = torch.max(outputs.data, 1)\n","        \n","    # Statistics\n","    total += labels.size(0)\n","    correct += torch.sum(predicted == labels.data)\n","\n","print('Accuracy on the test set: {:.2f}%'.format(100 * correct / total))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"voN-_iO_RQ8A"},"source":["The best results are obtained after 10 epochs!\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OFj_W39u5voa"},"source":["## Methods for improving training"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"chov57bzu76J"},"source":["### Batch normalization\n","Batch normalization is a trick that often yields faster training. It acts as a regularizer by normalizing the inputs (by batch). Further, the operation is differentiable.\n","\n","![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/1_Hiq-rLFGDpESpr8QNsJ1jg.png?raw=true)\n","\n","\n","For additional information see [article](https://arxiv.org/pdf/1502.03167v3.pdf).\n","\n","### Toolbox\n","Batch normalization is already implemented for us. To add it to our LeNet5 network, just add another layer in `__init__`. The following class can be used:\n","<ul>\n","<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.BatchNorm2d\">`nn.BatchNorm2d(num_features)`</a>: add batch normalisation to a 4-dimensional input encoded in a 3-dimensional tensor.</li>\n","</ul>\n","\n","### Implement\n","Below, you need to add batch normalization to the original LeNet5."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"6pi_mhvg8E4E"},"outputs":[],"source":["class LeNet5(nn.Module):\n","    def __init__(self):\n","        super(LeNet5, self).__init__()\n","        self.block1 = nn.Sequential(\n","            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2))\n","        \n","        self.block2 = nn.Sequential(\n","            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2))\n","        \n","        self.fc = nn.Linear(7*7*32, 10)\n","                \n","    def forward(self, x):\n","        out = self.block1(x)\n","\n","        out = self.block2(out)\n","        \n","        # Flatten the output of block2\n","        out = out.view(out.size(0), -1)\n","        \n","        out = self.fc(out)\n","               \n","        return out\n","        \n","model = LeNet5()\n","model = model.to(device)\n","  \n","print(model)\n","\n","print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"271HrQBNccH1"},"source":["Note that batch normalization adds parameters. Our new LeNet5 with batch normalization has 29 034 parameters (versus 28 938 for the original LeNet5 model without batch normalization)."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"vnbesUOudO_s"},"outputs":[],"source":["# Save the initial weights of model\n","init_model_wts = copy.deepcopy(model.state_dict())"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rj4R3qV5ABFC"},"source":["**The rest (i.e., the cost function, the optimizer, the training loops, and the testing procedures) remain unchanged!**"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"JgEcoSRQAVtA"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","model.load_state_dict(init_model_wts)\n","\n","since = time.time()\n","\n","num_epochs = 10\n","train_loss_history = []\n","valid_loss_history = []\n","\n","print(\"# Start training #\")\n","for epoch in range(num_epochs):\n","    \n","    train_loss = 0\n","    train_n_iter = 0\n","    \n","    # Set model to train mode\n","    model.train()\n","    \n","    # Iterate over train data\n","    for images, labels in train_loader:  \n","\n","        # put images on proper device (GPU)\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Zero the gradient buffer\n","        optimizer.zero_grad()  \n","        \n","        # Forward\n","        outputs = model(images)\n","        \n","        loss = criterion(outputs, labels)\n","        \n","        # Backward\n","        loss.backward()\n","        \n","        # Optimize\n","        optimizer.step()\n","        \n","        # Statistics\n","        train_loss += loss.item()\n","        train_n_iter += 1\n","    \n","    valid_loss = 0\n","    valid_n_iter = 0\n","    \n","    # Set model to evaluate mode\n","    model.eval()\n","    \n","    # Iterate over valid data\n","    for images, labels in valid_loader:  \n","        \n","        images = images.to(device)\n","        labels = labels.to(device)\n","        \n","        # Forward\n","        outputs = model(images)\n","        \n","        loss = criterion(outputs, labels)\n","        \n","      \n","        # Statistics\n","        valid_loss += loss.item()\n","        valid_n_iter += 1\n","    \n","    train_loss_history.append(train_loss / train_n_iter)\n","    valid_loss_history.append(valid_loss / valid_n_iter)\n","    \n","    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n","    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n","    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n","\n","time_elapsed = time.time() - since\n","\n","print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n","    time_elapsed // 60, time_elapsed % 60))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6pxvS_yUeWog"},"source":["We obtain even better results after 10 epochs!\n","\n","Let's have a look at the training and validation curves:"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"hrv-o7Cle0ty"},"outputs":[],"source":["# Save history for later\n","lenet5_batchnorm_train_loss_history = train_loss_history\n","lenet5_batchnorm_valid_loss_history = valid_loss_history\n","\n","# Plot training and validation curve\n","x = range(1, num_epochs + 1)\n","plt.plot(x, lenet5_train_loss_history, label='LeNet5 train')\n","plt.plot(x, lenet5_valid_loss_history, label='LeNet5 valid')\n","plt.plot(x, lenet5_batchnorm_train_loss_history, label='LeNet5 batch norm train')\n","plt.plot(x, lenet5_batchnorm_valid_loss_history, label='LeNet5 batch norm valid')\n","\n","plt.xlabel('# epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CHqHLCP6Crmp"},"source":["# Transfer Learning : finetuning a CNN\n","**Attribution:** this part of the tutorial is from: http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n","\n","In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. This is referred to as transfer learning.\n","\n","We will now explore the finetuning of a convolutionational network."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"itOkLFisvKqI"},"source":["## Downloading the data and creating the data loader\n","\n","We will use torchvision and torch.utils.data packages for loading the data.\n","\n","The problem we’re going to solve today is to train a model to classify ants and bees. We have about 120 training images each for ants and bees. There are 75 validation images for each class. Usually, this is a very small dataset to generalize upon, if trained from scratch. Since we are using transfer learning, we should be able to generalize reasonably well.\n","\n","This dataset is a very small subset of imagenet.\n","\n","Here are a few example images from our training dataset:\n","\n","![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/fourmi_abeille.png?raw=true)\n","\n","### Toolbox\n","**Recall:** an easy way to load the data in PyTorch involves: \n","<ol>\n","<li>Sublcass<a href=\"http://pytorch.org/docs/master/data.html#torch.utils.data.Dataset\">`torch.utils.data.Dataset`</a> and write the `__getitem__` and `__len__` methods. (Note that these do not load the data in memory.)</li>\n","<li>Use<a href=\"http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader\">`torch.utils.data.DataLoader`</a> to read and load the data into memory.</li>\n","</ol>\n","\n","**Note:** <a href=\"http://pytorch.org/docs/master/torchvision/datasets.html#torchvision-datasets\">`torchvision.datasets`</a> provides an alternative for loading data from a directory.\n","\n","### Implementation"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"_LW-1CauxEQM"},"outputs":[],"source":["%%bash\n","## DOWNLOAD DATASET ##\n","if [ ! -d \"hymenoptera_data\" ]; then\n","  wget --quiet https://download.pytorch.org/tutorial/hymenoptera_data.zip\n","  unzip -q hymenoptera_data.zip\n","  rm hymenoptera_data.zip\n","fi"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"OMvya9Oxps5z"},"outputs":[],"source":["import os\n","from PIL import Image\n","from torchvision import datasets\n","\n","\n","def make_dataset(root, split_type):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    root_dir : string\n","    Directory with all the images.\n","    split_type : string\n","    The name of the split in {'train', 'valid'}.\n","\n","    Returns\n","    -------\n","    images : dict\n","    Dict of images path for each classes for a specific split type.\n","    \"\"\"\n","\n","    images = {}\n","    root = os.path.join(root, split_type)\n","\n","    for classes in sorted(os.listdir(root)):\n","        images[classes] = []\n","        path_classes = os.path.join(root, classes)\n","\n","        for root_, _, fnames in sorted(os.walk(path_classes)):\n","            for fname in sorted(fnames):\n","                if fname.endswith('.jpg'):\n","                    item = os.path.join(root_, fname)\n","                    images[classes].append(item)\n","\n","    return images\n","\n","\n","class HymenopteraDataset(torch.utils.data.Dataset):\n","    \"\"\"Hymenoptera dataset.\"\"\"\n","\n","    def __init__(self, root_dir, split_type='train', transform=None):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        root_dir : string\n","           Directory with all the images.\n","        split_type : string\n","           The name of the split in {'train', 'valid', 'test', 'train_valid'}.\n","        transform : callable, optional\n","           Optional transform to be applied on a sample.\n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.split_type = split_type\n","        self.transform = transform\n","        self.classes = {'ants': 0, 'bees': 1}\n","\n","        imgs_ = []\n","        target_ = []\n","\n","        if split_type == 'train':\n","            imgs = make_dataset(root_dir, 'train')\n","            for k, v in imgs.items():\n","                imgs_ += imgs[k][:int(0.8*len(v))]\n","                target_ += [self.classes[k]] * len(imgs_)\n","\n","        elif split_type == 'valid':\n","            imgs = make_dataset(root_dir, 'train')\n","            for k, v in imgs.items():\n","                imgs_ += imgs[k][int(0.8*len(v)):]\n","                target_ += [self.classes[k]] * len(imgs_)\n","\n","        elif split_type == 'train_valid':\n","            imgs = make_dataset(root_dir, 'train')\n","            for k, v in imgs.items():\n","                imgs_ += imgs[k]\n","                target_ += [self.classes[k]] * len(imgs_)\n","\n","        elif split_type == 'test':\n","            imgs = make_dataset(root_dir, 'val')\n","            for k, v in imgs.items():\n","                imgs_ += imgs[k]\n","                target_ += [self.classes[k]] * len(imgs_)\n","\n","        self.imgs = imgs_\n","        self.target = np.array(target_)\n","\n","    def __len__(self):\n","        \"\"\"Get the number of image in the dataset.\n","\n","        Returns\n","        -------\n","        int\n","           The number of images in the dataset.\n","        \"\"\"\n","        return len(self.imgs)\n","\n","    def __getitem__(self, index):\n","        \"\"\"Get the items : image, target\n","\n","        Parameters\n","        ----------\n","        index : int\n","           Index\n","\n","        Returns\n","        -------\n","        img : tensor\n","           The image.\n","        target : int\n","           Target is class_index of the target class.\n","        \"\"\"\n","        path = self.imgs[index]\n","        target = self.target[index]\n","\n","        with open(path, 'rb') as f:\n","            with Image.open(f) as img:\n","                img.convert('RGB')\n","\n","                if self.transform:\n","                    img = self.transform(img)\n","\n","        return img, target\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"c9BHo5WsOghr"},"source":["## Data augmentation\n","\n","Data augmentation is a trick which can be used to augment the effective size of your training data. This can, in turn, help reduce overfitting. It consists in creating new data by transforming available training data. For example for images, we can resize, stretch, rotate, mirror them to obtain additional images. Here are some examples of image augmentations (right) from an original image (left): \n","\n","![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/1_Jujct_Pt-zvdWtSFpHUp3Q.png?raw=true)\n","\n","\n","To augment your data, <a href=\"http://pytorch.org/docs/master/torchvision/transforms.html#torchvision-transforms\">`torchvision.transforms`</a> \n","provides common image transformations. These transformations can be applied successively by using<a href=\"http://pytorch.org/docs/master/torchvision/transforms.html#torchvision.transforms.Compose\">`torchvision.transforms.Compose`</a>.\n","\n","Add the following transformations using your training data:\n","* Random crop to resize each image to size 224x224\n","* Some probability of taking a mirror of the image \n","* An image normalization with the following means and standard deviations: [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n","\n","For the validation data, resize the image to have size 256x256, take a crop starting from the center and normalize the image with the same values as used for the training data."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"v94wZLuOp4m3"},"outputs":[],"source":["# Data augmentation and normalization for training\n","# Just normalization for validation\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'valid': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"8i9eB9I_p5sJ"},"outputs":[],"source":["# Dataset loader\n","data_dir = 'hymenoptera_data'\n","\n","data_train = HymenopteraDataset(data_dir, 'train', data_transforms['train'])\n","train_loader = DataLoader(data_train, batch_size=4, shuffle=True, num_workers=4)\n","\n","data_valid = HymenopteraDataset(data_dir, 'valid', data_transforms['valid'])\n","valid_loader = DataLoader(data_valid, batch_size=4, shuffle=False, num_workers=4)\n","\n","data_test = HymenopteraDataset(data_dir, 'test', data_transforms['valid'])\n","test_loader = DataLoader(data_test, batch_size=4, shuffle=False, num_workers=4)\n","\n","print('# images in data train: {}'.format(len(data_train)))\n","print('# images in data valid: {}'.format(len(data_valid)))\n","print('# images in data test: {}'.format(len(data_test)))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Pl1X1vt5AVaf"},"source":["Let's have a look at our training data!"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"f4kbOO5XAUO5"},"outputs":[],"source":["inputs, classes = next(iter(train_loader))\n","\n","print('Classes: {}'.format(data_train.classes))\n","print('Inputs size: {}'.format(inputs.size()))\n","print('Classes size: {}'.format(classes.size()))\n","\n","# Random image of the batch\n","idx = np.random.randint(len(inputs))\n","img = inputs[idx]\n","labels = list(data_train.classes.keys())\n","img_label = labels[(classes[idx])]\n","\n","img = img.numpy().transpose((1, 2, 0))\n","mean = np.array([0.485, 0.456, 0.406])\n","std = np.array([0.229, 0.224, 0.225])\n","img = std * img + mean\n","img = np.clip(img, 0, 1)\n","plt.imshow(img)\n","plt.title(img_label)\n","plt.grid(False)\n","plt.show()"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"qh3bKAO7Lf14"},"outputs":[],"source":["import torchvision\n","\n","def imshow(img, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    img = img.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    img = std * img + mean\n","    img = np.clip(img, 0, 1)\n","    plt.imshow(img)\n","    if title is not None:\n","        plt.title(title)\n","    plt.grid(False)\n","    plt.show()\n","\n","inputs, classes = next(iter(train_loader))\n","\n","out = torchvision.utils.make_grid(inputs)\n","\n","class_names = data_train.classes\n","class_names = {class_names[k]: k for k in class_names.keys()}\n","\n","imshow(out, title=[class_names[int(x)] for x in classes])"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gN9JbYASp4tE"},"source":["## Determing the network's architecture\n","### Toolbox\n","\n","We will reuse a network pre-trained on ImageNet. To do so, we first load the pre-trained model and reinitialize its final layer (the fully connected one). Luckily, in PyTorch <a href=\"http://pytorch.org/docs/0.1.12/torchvision/models.html#module-torchvision.models\">`torchvision.models`</a>\n","proposes pre-trained models on ImageNet. \n","\n","A common choice for classification problem (our setting) is to use the *ResNet18* model. For more information regarding this model, see [article](https://arxiv.org/abs/1512.03385). Useful information is also provided in the PyTorch documentation of the model:\n","<a href=\"http://pytorch.org/docs/0.1.12/torchvision/models.html#torchvision.models.resnet18\">`torchvision.models.resnet18(pretrained=True)`</a>\n","\n","Here is an example of a residual bloc (residual blocks are the core of residual networks such as *ResNet18*).\n","\n","![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/bloc_residuel.png?raw=true)\n","\n","\n","**Recall:** <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Linear\">`torch.nn.Linear(in_features, out_features)`</a> performs a linear transformation to input data x: y = Ax + b.\n","\n","### Implement"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TiQfdQxiMJPQ"},"source":["To gain insights into the value of pre-trained networks, we will first train a network from scratch and then compare it to a pre-trained version of the same model."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"nxBgVe4NISea"},"outputs":[],"source":["from torchvision import models\n","\n","# Load non-pre-trained resnet18 model\n","model = models.resnet18(pretrained=False)\n","\n","# Reset last layer\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 2)\n","\n","model = model.to(device)\n","\n","print(model)\n","\n","print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"vQTyN1tkZyYG"},"outputs":[],"source":["# Save the initial weights of model\n","init_model_wts = copy.deepcopy(model.state_dict())"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"x_ut6uP0qpWW"},"source":["## Determine the cost function and the optimization method\n","### Toolbox\n","**Recall:** A common choice for a multi-class task are the following:\n","<ul>\n","<li>**Cost function :** <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss\">`torch.nn.CrossEntropyLoss()`</a>. The cross entropy is often used in this context. It compares a (multivariate) distribution $p$ with a reference distribution $t$. It is minimized for $p=t$ and it is expressed mathematically by: $-\\sum_j t_{ij} \\log(p_{ij})$ where $p$ is the prediction,, $t$ the target, $i$ are examples and $j$ the target class.</li>\n","<li>**Optimization method :** <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD(net.parameters(), lr=learning_rate)`</a> a standard stochastic gradient descent (SGD) implementation. Here use a learning rate of $1e-3$ and a momentum of $0.9$</li>\n","</ul>\n","\n","### Implementation"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"JW00K1Ssqqcm"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","\n","learning_rate = 1e-3\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9uhoTb410Utx"},"source":["## Training a network\n","### Toolbox\n","\n","**Recall:** Training a network usually involves iterating for multiple epochs over the training dataset. One epoch corresponds to one pass over the full dataset. The dataset is usually divided into batches. Each epoch will then receive sequentially batches. For each batch we do the following operations:\n","<ol>\n","<li>`optimizer.zero_grad()`: we clear the previously stored gradients.</li>\n","<li>`loss.backward()`: we evaluate the cost, the gradients and backpropagate the gradients through the computation graph.</li>\n","<li>`optimizer.step()`: we update the parameters using the previously calculated gradients. For SGD, the update is: `weight = weight - learning_rate * gradient`. Adam is similar but also adds bells and whistles (e.g., it adapts the learning rate over time for each parameter)</li>\n","</ol>\n","\n","**Bonus:** To train deep neural networks we often use these additional tricks:\n","<ul>\n","<li>early stopping: monitors the validation error of the model and stops training if it begins to overfit (e.g., if it stops improving).</li>\n","<li>checkpointing: To do so, it is common to save the network's weight (you can obtain them using `model.state_dict()`) throughout training.</li>\n","<li>printing execution time. To do so, you can use `time.time()`.</li>\n","</ul>\n","\n","### Implementation"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"hXNpyIj_wxRP"},"outputs":[],"source":["import time\n","\n","since = time.time()\n","\n","best_model_wts = copy.deepcopy(model.state_dict())\n","\n","num_epochs = 25\n","best_acc = 0.0\n","\n","train_loss_history = []\n","valid_loss_history = []\n","\n","print(\"# Start training #\")\n","for epoch in range(num_epochs):\n","    \n","    train_loss = 0\n","    train_n_iter = 0\n","    \n","    # Set model to train mode\n","    model.train()\n","    \n","    # Iterate over train data\n","    for images, labels in train_loader:  \n","        \n","        # put images on proper device (GPU)\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Zero the gradient buffer\n","        optimizer.zero_grad()  \n","        \n","        # Forward\n","        outputs = model(images)\n","        \n","        loss = criterion(outputs, labels)\n","        \n","        # Backward\n","        loss.backward()\n","        \n","        # Optimize\n","        optimizer.step()\n","        \n","        # Statistics\n","        train_loss += loss.item()\n","        train_n_iter += 1\n","    \n","    valid_loss = 0\n","    valid_n_iter = 0\n","    \n","    # Set model to evaluate mode\n","    model.eval()\n","    \n","    # Iterate over valid data\n","    total = 0\n","    correct = 0\n","    for images, labels in valid_loader:  \n","        \n","        # put images on proper device (GPU)\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        \n","        loss = criterion(outputs, labels)\n","    \n","        # Statistics\n","        total += labels.size(0)\n","        correct += torch.sum(predicted == labels.data)\n","        valid_loss += loss.item()\n","        valid_n_iter += 1\n","    \n","    epoch_acc = 100 * correct / total\n","    \n","    # Deep copy the best model\n","    if epoch_acc > best_acc:\n","        best_acc = epoch_acc\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","    \n","    train_loss_history.append(train_loss / train_n_iter)\n","    valid_loss_history.append(valid_loss / valid_n_iter)\n","    \n","    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n","    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n","    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n","\n","time_elapsed = time.time() - since\n","\n","print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n","    time_elapsed // 60, time_elapsed % 60))\n","\n","print('\\n\\nBest valid accuracy: {:.2f}'.format(best_acc))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Xr8-PgSmiQSF"},"source":["Let's visualize the training and validation curves:"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"njb7oaT-hfk3"},"outputs":[],"source":["resnet18_train_loss_history = train_loss_history\n","resnet18_valid_loss_history = valid_loss_history\n","\n","# Plot training and validation curve\n","x = range(1, num_epochs + 1)\n","plt.plot(x, resnet18_train_loss_history, label='ResNet18 train')\n","plt.plot(x, resnet18_valid_loss_history, label='ResNet18 valid')\n","\n","plt.xlabel('# epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Vcqa0H5p-FHq"},"source":["## Testing the network\n","### Toolbox\n","**Recall:** we evaluate the network's performance on test data.\n","\n","**Note:** Here, we do not have a test set so we will use the validation set instead (DO NOT DO THIS IN PRACTICE).\n","\n","**Using the best model:** we want to reuse the weights from the best model to evaluate it on a new dataset (here validation data). These weights have been saved in the training phase in `best_model_wts`. To load them, you can use `model.load_state_dict(best_model_wts)`.\n","\n","### Implementation"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"C1DCCblW_EPo"},"outputs":[],"source":["# Load best model weights\n","model.load_state_dict(best_model_wts)\n","\n","# Set model to evaluate mode\n","model.eval()\n","\n","correct = 0\n","total = 0\n","\n","# Iterate over test data\n","for images, labels in test_loader:\n","    \n","    # put images on proper device (GPU)\n","    images = images.to(device)\n","    labels = labels.to(device)\n","    \n","    # Forward\n","    outputs = model(images)\n","    _, predicted = torch.max(outputs.data, 1)\n","    \n","    # Statistics\n","    total += labels.size(0)\n","    correct += torch.sum(predicted == labels.data)\n","\n","print('Accuracy on the test set: {:.2f}%'.format(100 * correct / total))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"d38Q32VMN0Hb"},"source":["With pre-trained weights:"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"qMbvwaR5Ny-K"},"outputs":[],"source":["from torchvision import models\n","\n","# Load pre-trained resnet18 model\n","model = models.resnet18(pretrained=True)\n","\n","# Reset last layer\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 2)\n","\n","model = model.to(device)\n","\n","print(model)\n","\n","print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"XAcPL09nN-Ki"},"outputs":[],"source":["# Save the initial weights of model\n","init_model_wts = copy.deepcopy(model.state_dict())"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"88Nhc4mlN-sc"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"U1pQBFbAOEjd"},"outputs":[],"source":["since = time.time()\n","\n","best_model_wts = copy.deepcopy(model.state_dict())\n","\n","num_epochs = 25\n","best_acc = 0.0\n","\n","train_loss_history = []\n","valid_loss_history = []\n","\n","print(\"# Start training #\")\n","for epoch in range(num_epochs):\n","    \n","    train_loss = 0\n","    train_n_iter = 0\n","    \n","    # Set model to train mode\n","    model.train()\n","    \n","    # Iterate over train data\n","    for images, labels in train_loader:  \n","        \n","        # put images on proper device (GPU)\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Zero the gradient buffer\n","        optimizer.zero_grad()  \n","        \n","        # Forward\n","        outputs = model(images)\n","        \n","        loss = criterion(outputs, labels)\n","        \n","        # Backward\n","        loss.backward()\n","        \n","        # Optimize\n","        optimizer.step()\n","        \n","        # Statistics\n","        train_loss += loss.item()\n","        train_n_iter += 1\n","    \n","    valid_loss = 0\n","    valid_n_iter = 0\n","    \n","    # Set model to evaluate mode\n","    model.eval()\n","    \n","    # Iterate over valid data\n","    total = 0\n","    correct = 0\n","    for images, labels in valid_loader:  \n","        \n","        # put images on proper device (GPU)\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        \n","        # Forward\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        \n","        loss = criterion(outputs, labels)\n","    \n","        # Statistics\n","        total += labels.size(0)\n","        correct += torch.sum(predicted == labels.data)\n","        valid_loss += loss.item()\n","        valid_n_iter += 1\n","    \n","    epoch_acc = 100 * correct / total\n","    \n","    # Deep copy the best model\n","    if epoch_acc > best_acc:\n","        best_acc = epoch_acc\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","    \n","    train_loss_history.append(train_loss / train_n_iter)\n","    valid_loss_history.append(valid_loss / valid_n_iter)\n","    \n","    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n","    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n","    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n","\n","time_elapsed = time.time() - since\n","\n","print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n","    time_elapsed // 60, time_elapsed % 60))\n","\n","print('\\n\\nBest valid accuracy: {:.2f}'.format(best_acc))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"d_x_Kot7iEnI"},"source":["Let's have a look at train and validation error curves:"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"sVUPxQCehyzc"},"outputs":[],"source":["resnet18_pretrained_train_loss_history = train_loss_history\n","resnet18_pretrained_valid_loss_history = valid_loss_history\n","\n","# Plot training and validation curve\n","x = range(1, num_epochs + 1)\n","plt.plot(x, resnet18_train_loss_history, label='ResNet18 train')\n","plt.plot(x, resnet18_valid_loss_history, label='ResNet18 valid')\n","plt.plot(\n","      x, resnet18_pretrained_train_loss_history,\n","    label='ResNet18 pretrained train')\n","plt.plot(\n","      x, resnet18_pretrained_valid_loss_history,\n","    label='ResNet18 pretrained valid')\n","\n","plt.xlabel('# epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"keq8hL3yiCOG"},"source":["Let's test the model:"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"MJy4FdfQfc7z"},"outputs":[],"source":["# Load best model weights\n","model.load_state_dict(best_model_wts)\n","\n","# Set model to evaluate mode\n","model.eval()\n","\n","correct = 0\n","total = 0\n","\n","# Iterate over test data\n","for images, labels in test_loader:\n","    \n","    # put images on proper device (GPU)\n","    images = images.to(device)\n","    labels = labels.to(device)\n","        \n","    # Forward pass\n","    outputs = model(images)\n","    _, predicted = torch.max(outputs.data, 1)\n","    \n","    # Statistics\n","    total += labels.size(0)\n","    correct += torch.sum(predicted == labels.data)\n","\n","print('Accuracy on the test set: {:.2f}%'.format(100 * correct / total))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LMd3mx5KKYXk"},"source":["Note that we obtain a better accuracy on the test versus the model with the weights that hadn't been trained."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"u69CX2svt_4l"},"source":["# Follow-up (optional)\n","\n","If you wish to know more about convolutional neural networks here are a few pointers to more advanced tasks that can be performed with these models:\n","\n","\n","## Image segmentation\n","\n","While image classification is useful, it can be even more useful to determine where a particular object of interest is in the image. This is called image segmentation (it is trying to segment the image into various regions). A popular algorithm for image segmentation is [Mask R-CNN](https://arxiv.org/abs/1703.06870). Here is a tutorial that goes over the implementation of this algorithm [here].(https://github.com/matterport/Mask_RCNN/blob/master/samples/shapes/train_shapes.ipynb)\n","\n","Here is an example of a segmented image: \n","\n","![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/detection_final.png?raw=true)\n","\n","\n","## Generative models (GAN example)\n","\n","A generative adversarial network (GAN) consists of two networks that are \"playing\" against one another. Each is trying to fool the other. The first, the generator, tries to generate realistic data (e.g., images) while the second, the discriminator, tries to determine whether the generated data is real or fake. \n","\n","Training both of these networks together can lead to the generation of high-quality data (often images). \n","For example, here is an MNIST-list digit generated from a GAN trained model:\n","![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/1_nAVqFluPijpBWR2tI4gCxg.png?raw=true)\n","\n","More information is available in the original [GAN paper](http://papers.nips.cc/paper/5423-generative-adversarial-nets). [Here](https://github.com/diegoalejogm/gans) is an implementation of it.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IsLwCjnbWkk4"},"source":["# References\n","Various parts of this tutorial are inspired from these other tutorials: \n","<ul>\n","<li>https://github.com/andrewliao11/dni.pytorch/blob/master/mlp.py\n","<li>https://github.com/andrewliao11/dni.pytorch/blob/master/cnn.py\n","<li>http://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n","<li>http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n"," <li>http://cs231n.github.io/convolutional-networks/\n"," <li>http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html#convolution-as-a-matrix-operation\n","</ul>"]}]}
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_5PWr7JoIbf"
   },
   "source": [
    "source: https://github.com/mila-iqia/ivado-mila-dl-school-2019/tree/master/tutorials\n",
    "### Authors: \n",
    "\n",
    "Margaux Luck <margaux.luck@rd.mila.quebec>\n",
    "\n",
    "Jeremy Pinto <jeremy.pinto@rd.mila.quebec>\n",
    "\n",
    "Mathieu Germain <mathieu.germain@rd.mila.quebec>\n",
    "\n",
    "### Translation to English: \n",
    "\n",
    "Laurent Charlin <lcharlin@gmail.com>\n",
    "\n",
    "### Intro\n",
    "\n",
    "This tutorial uses concrete examples to introduce the fundamental concepts behind convolutional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A73O9J3necuk"
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PNXwEm-ej9a"
   },
   "source": [
    "\n",
    "Before we begin, you must ensure to install all required libraries for this tutorial. To do so, we will use the `pip` utility. Simply execute the cell below by selecting it and pressing `shift`+`Enter`. (This operation may take a few minutes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNlYWG6z9GKT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision Pillow matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ddGNnGb0lMFJ"
   },
   "source": [
    "To ensure that all required libraries are available, let's try to load all libraries and modules we will need during this tutorial by executing this cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJzEY8Q2XQbL"
   },
   "outputs": [],
   "source": [
    "#import imp.find_module('torch')\n",
    "import importlib\n",
    "required_libraries = ['torch', 'torchvision', 'pillow', 'matplotlib']\n",
    "for lib in required_libraries:\n",
    "    if importlib.util.find_spec(lib) is None:\n",
    "        print(\"%s unavailable\" % lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QfmCDK-xlRmW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
    "\n",
    "print(\"Torch version: \", torch.__version__)\n",
    "print(\"GPU Available: {}\".format(use_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FNomSpLO9LeH"
   },
   "source": [
    "# The MNIST dataset\n",
    "MNIST is the classic go-to **classification dataset** used in **computer vision**. It is available here: <a href=\"http://yann.lecun.com/exdb/mnist/\">Yann LeCun's website</a>. \n",
    "\n",
    "Each datum is an **image of a handwritten digit**. Here are a few examples from this dataset: \n",
    "\n",
    "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/mnist.png?raw=true)\n",
    "\n",
    "Each image also comes with a **class label** which indicates which digit does the image correspond to. For example, the labels of the examples above are 5, 0, 4, and 1 respectively. The classes are balanced which means that all digits appear (roughly) the same number of times in the dataset.\n",
    "\n",
    "The dataset is composed of **60 000 training examples** and **10 000 test examples**. All images have exactly the same size (**28x28 pixels** or 28 rows by 28 columns). Each pixel is represented by a number between 0 and 255 which represents its grey level (0 is white and 255 is black). Depending on the model, each image may have to be flattened (to a 784-length vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wk9AMaT9hmcX"
   },
   "source": [
    "## Objective\n",
    "\n",
    "Our goal is to find a model that correctly classifies these images. In particular, our model will take as input (features/covariates/independant-variables) an image and will predict its digit (label/dependant variable). This problem can be formalized as follows:\n",
    "\n",
    "`f(image) = predicted digit`\n",
    "\n",
    "where `f` is a function.\n",
    "\n",
    "In this tutorial, we will consider both **multi-layer perceptrons (MLPs)** and **convolutional neural networks** as functions for solving this prediction problem. Both models take as inputs pixel intensities which will be modified using mathematical operations through the layers of the networks. Their output is a vector of size *1x10* where each entry corresponds to the (normalized) score that the input image is a particular digit. The sum over these 10 values is 1, and each score is non-negative. This is why these scores can be interpreted as probabilities. Our final prediction will be the entry with the highest score. For example, this prediction\n",
    "\n",
    "`[0.8, 0.1, 0, 0, 0, 0.05, 0.05, 0.0, 0.0, 0.0, 0.0]`\n",
    "\n",
    "indicates that the model assigns a score of 0.8 to class 0.\n",
    "\n",
    "Learning implies finding the parameters of a model that will maximize the model's performance. To learn, we will start by randomly initializing the parameters of our model. Then we iterate through examples. For each example we will obtain the network's prediction, compare it with the true label, and then update the parameters of the models to obtain a better prediction. We do this until we reach some predetermined stopping criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1U2sDz9ufy9"
   },
   "source": [
    "## Download the dataset and create the data loader\n",
    "\n",
    "Before we begin training, we have to obtain the MNIST dataset. It turns out that there are built-in functions within PyTorch to do so.\n",
    "\n",
    "### Tool box\n",
    "**Note:** PyTorch comes with function to load, shuffle, and augment data.\n",
    "\n",
    "Here is an easy way to load the data in PyTorch: \n",
    "<ol>\n",
    "<li>Subclass <a href=\"http://pytorch.org/docs/master/data.html#torch.utils.data.Dataset\">`torch.utils.data.Dataset`</a> and add  `__getitem__` and `__len__` methods.</li>\n",
    "<li>Then you can use<a href=\"http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader\">`torch.utils.data.DataLoader`</a> to read and load the data into memory.</li>\n",
    "</ol>\n",
    "\n",
    "It is even easier for MNIST in PyTorch since there is already a subclass of \"datasets\" defined for it: <a href=\"http://pytorch.org/docs/master/torchvision/datasets.html#mnist\">`torchvision.datasets.MNIST`</a>.\n",
    "\n",
    "<a href=\"http://pytorch.org/docs/master/torchvision/datasets.html\">Other datasets are also similarly available</a>\n",
    "\n",
    "**Note:** <a href=\"http://pytorch.org/docs/master/tensors.html#torch.Tensor.view\">`torch.Tensor.view()`</a> returns a new tensor with the same data as the original tensor but a different shape. For example, it can be used to flatten an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2iC4F8H8bsx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import sampler, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "manualSeed = 1234\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# Fixing random seed\n",
    "random.seed(manualSeed)\n",
    "np.random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "if use_gpu:\n",
    "   torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset.\n",
    "    From: https://github.com/pytorch/vision/issues/168\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_samples: int\n",
    "      # of desired datapoints\n",
    "    start: int\n",
    "      Offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start=0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "\n",
    "train_dataset = MNIST(root='../data', \n",
    "                      train=True, \n",
    "                      transform=transforms.ToTensor(),  \n",
    "                      download=True)\n",
    "\n",
    "test_dataset = MNIST(root='../data', \n",
    "                     train=False, \n",
    "                     transform=transforms.ToTensor())\n",
    "\n",
    "train_dataset_sizes = len(train_dataset)\n",
    "num_train_samples = int(0.8 * train_dataset_sizes)\n",
    "num_valid_samples = train_dataset_sizes - num_train_samples\n",
    "num_test_samples = len(test_dataset)\n",
    "\n",
    "print('# of train examples: {}'.format(num_train_samples))\n",
    "print('# of valid examples: {}'.format(num_valid_samples))\n",
    "print('# of test examples: {}'.format(num_test_samples))\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          sampler=ChunkSampler(num_train_samples, 0),\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False)\n",
    "\n",
    "valid_loader = DataLoader(dataset=train_dataset,\n",
    "                          sampler=ChunkSampler(\n",
    "                              num_valid_samples, num_train_samples),\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gRV8zZbHV6zN"
   },
   "source": [
    "Let's visualize the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TB57DZYzV9Oz"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inputs, classes = next(iter(train_loader))\n",
    "\n",
    "print('Inputs size: {}'.format(inputs.size()))\n",
    "print('Classes size: {}'.format(classes.size()))\n",
    "\n",
    "# Random image of the batch\n",
    "img1 = 255 - inputs[np.random.randint(len(inputs))] * 255\n",
    "\n",
    "# Plot the image\n",
    "print('\\n\\nDisplay the first image:')\n",
    "img1 = img1.numpy()[0, :, :]\n",
    "plt.imshow(img1, cmap='gray', vmin=0, vmax=255)\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_XSUTbG0UvX"
   },
   "source": [
    "# CPU or GPU\n",
    "**Note:** <a href=\"http://pytorch.org/docs/master/cuda.html#module-torch.cuda\">`torch.cuda`</a> is a library which can perform tensor operations using GPUs. Specifically, the library includes CUDA tensors which offer the same operations as regular tensors but instead run on GPUs, instead of CPUs.\n",
    "<a href=\"http://pytorch.org/docs/master/cuda.html#torch.cuda.is_available\">`torch.cuda.is_available()`</a> returns whether or not CUDA is available. Adding `.to(\"cuda:0\")` to the variable identifying a CPU tensor, returns the equivalent GPU tensor.\n",
    "\n",
    "For more information about using GPUs on colab, please refer to this [tutorial](https://colab.research.google.com/drive/1y3ZE4m-D7lPoMzsypSEXessYmjWfKGqD#scrollTo=3IEVK-KFxi5Z).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxnZv9g_0RQK"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "print(\"GPU Available: {}\".format(use_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xjWUALQGXQbi"
   },
   "source": [
    "We are going to compare the performance of a CNN to the performance of a vanilla MLP. Below is a quick reminder of MLPs, the architecture of the MLP we will be using, as well as the code to train the MLP. \n",
    "\n",
    "# Multi-layer perceptron (MLP)\n",
    "A multi-layer perceptron is a vanilla few-forward neural network. Our instanciation will take as input an image, will transform it through a series of hidden layers and then will pass it to an output layer. This output is a vector of 10 numbers where each represents the normalized score of a particular class (this is sometimes interpreted as a probability).\n",
    "\n",
    "For example, here an MLP architecture to classify MNIST images: \n",
    "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/mlp.png?raw=true)\n",
    "\n",
    "Whenever you are trying to solve a prediction task, the process usually goes as follows: \n",
    "<ol>\n",
    "<li>Determine the network's artchitecture. This will implicitely determine the number of parameters (weights and biases) of the network.</li>\n",
    "<li>Determine the cost function and the optimization method.</li>\n",
    "<li>Train the weights of the network (i.e., fit the model to train data).</li>\n",
    "<li>Test the network (i.e., evaluate its performance on test data).</li>\n",
    "</ol>\n",
    "\n",
    "This procedure is general and applies to all types of (deep) neural networks.\n",
    "\n",
    "### Toolbox\n",
    "\n",
    "Racall that a (deep) neural network can be coded by using the library <a href=\"http://pytorch.org/docs/master/nn.html\">`torch.nn`</a>. `nn` uses <a href=\"http://pytorch.org/docs/master/autograd.html\">`torch.autograd`</a> to instantiate and computer the gradients (of the loss function with respect to the parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ho71dy2JXQbk"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, num_classes))\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        \n",
    "        out = self.hidden_layer(x)\n",
    "        \n",
    "        out = self.output_layer(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "model = MLP(input_size, hidden_size, num_classes)\n",
    "# switch model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "# Save the initial weights of model\n",
    "init_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bdh8ocHBXQbn"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.autograd import Variable\n",
    "\n",
    "model.load_state_dict(init_model_wts)\n",
    "\n",
    "since = time.time()\n",
    "\n",
    "num_epochs = 10\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "print(\"# Start training #\")\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_n_iter = 0\n",
    "    \n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over train data\n",
    "    for images, labels in train_loader:  \n",
    "        \n",
    "        # put images on proper device (GPU)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Flatten the images\n",
    "        images = images.view(-1, 28*28)\n",
    "\n",
    "        # Zero the gradient buffer\n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        train_n_iter += 1\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_n_iter = 0\n",
    "    \n",
    "    # Set model to evaluate mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Iterate over valid data\n",
    "    for images, labels in valid_loader:  \n",
    "        \n",
    "        # put images on proper device (GPU)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Flatten the images\n",
    "        images = images.view(-1, 28*28)\n",
    "        \n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Statistics\n",
    "        valid_loss += loss.item()\n",
    "        valid_n_iter += 1\n",
    "    \n",
    "    train_loss_history.append(train_loss / train_n_iter)\n",
    "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
    "    \n",
    "    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n",
    "    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n",
    "    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "\n",
    "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UhO4qn_vXQbq"
   },
   "outputs": [],
   "source": [
    "# Save history for later\n",
    "mlp_train_loss_history = train_loss_history\n",
    "mlp_valid_loss_history = valid_loss_history\n",
    "\n",
    "# Plot training and validation curve\n",
    "x = range(1, num_epochs + 1)\n",
    "plt.plot(x, mlp_train_loss_history, label='train')\n",
    "plt.plot(x, mlp_valid_loss_history, label='valid')\n",
    "\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3W5NEuERXQbt"
   },
   "outputs": [],
   "source": [
    "# Set model to evaluate mode\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Iterate over test data\n",
    "for images, labels in test_loader:\n",
    "    \n",
    "    # put images on proper device (GPU)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Flatten the images\n",
    "    images = images.view(-1, 28*28)\n",
    "    \n",
    "\n",
    "    # Forward\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # Statistics\n",
    "    total += labels.size(0)\n",
    "    correct += torch.sum(predicted == labels.data)\n",
    "\n",
    "print('Accuracy on the test set: {:.2f}%'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hp1YgKZb8btG"
   },
   "source": [
    "# Convolutional neural networks (CNNs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMbU21dg3Rqx"
   },
   "source": [
    "## Concepts\n",
    "\n",
    "We first review the basic concepts that underlie CNNs.\n",
    "\n",
    "### Convolution\n",
    "\n",
    "A convolution \"slides\" a filter *K* along image *I* to obtain an output *I*\\**K*.\n",
    "\n",
    "Here is an example of a 2D convolution:\n",
    "\n",
    "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/numerical_no_padding_no_strides.gif?raw=true)\n",
    "\n",
    "\n",
    "### Filters\n",
    "\n",
    "Filters (or kernels) are used to extract information useful to the task from their input. Filters are generally of size *n* \\* *n* where *n* is usually odd. The filters are parametrized by weights, one for each of its entry, which are learned by the convolutional network.\n",
    "\n",
    "The filter used in the previous example is:\n",
    "\n",
    "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/d7acc4aeb74d9e9cb5fb51482a302196594837fe.png?raw=true)\n",
    "\n",
    "### Depth\n",
    "\n",
    "We typically use *M* of filters which can be understood as the depth of the layer (see below). Note that this is different from the depth of the network (which is the number of layers). M is a hyperparameter. Here, each filter's output (blue circles) is represented as a single depth dimension on the output.\n",
    "\n",
    "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/Conv_layer.png?raw=true)\n",
    "\n",
    "### Stride\n",
    "\n",
    "The stride corresponds to the number of pixels the filter moves over in between each step of the convolution. The stride is measured in terms of a number of pixels. We typically use strides of 1 or 2. The larger the stride, the smaller the dimension of the output.\n",
    "\n",
    "### Zero padding\n",
    "\n",
    "Zero padding consists in padding (adding) a border of zeros around the input image. This can be useful to preserve the dimension from input to output.\n",
    "\n",
    "Below is an example of a zero padding which preserves the dimensions from input to output. Here, zero padding is set to 1, stride is set to 1, and the filter has size 3x3.\n",
    "\n",
    "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/same_padding_no_strides.gif?raw=true)\n",
    "\n",
    "\n",
    "### Max Pooling\n",
    "\n",
    "In addition to convolutions, CNNs usually have pooling layers. The goal of pooling is to reduce the dimensionality of the input in-between two convolution layers to reduce the number of parameters in the network. For example, the famous LeNet CNN, uses max pooling with 2x2 filters and a stride of 2. Max pooling outputs the max value in a 2x2 region. This output is then the input of the next layer.\n",
    "\n",
    "Here is an example of the max pooling operation:\n",
    "\n",
    "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/maxpool.jpeg?raw=true)\n",
    "\n",
    "\n",
    "### Receptive Field\n",
    "\n",
    "The receptive field is a measure of the CNNs' capacity to perceive information at different input scales. In an MLP (fully connected), the features are connected to every neuron. The receptive field of this MLP is the full image.\n",
    "\n",
    "For CNNs, convolution operations typically imply sparse connections. In other words, each filter only has a local receptive field. However, each successive layer will have access to a slightly larger receptive field. \n",
    "\n",
    "Let's look at an example. Imagine a 3x3 filter with stride set to 1. In this case, the first layer's receptive field is a maximum of 3x3. However, the more layer we add the more we increase the network's receptive field. Adding a second layer with 3xe3 filters and a stride of 1, our receptive field is increased to 5x5. Adding a third 3x3 layer further increases our receptive field to 7x7.\n",
    "\n",
    "What is the advantage of using multiple smaller successive filters instead of a single larger one? A single large filter of 7x7 implies 49 parameters. Instead 3 layers of 3x3 filters requires only 27 parameters (9 \\* 3). It is therefore more efficient to use multiple successive filters and in both cases the receptive field is the same (7x7). In addition, by using multiple successive filters, we can introduce more non-linearities in the model (one after each filter).\n",
    "\n",
    "Here the 3x3 filter (in grey) with a stride of 1 has a receptive field of 5x5 (yellow region):\n",
    "\n",
    "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/sNBmKMKAz-yJeCuS14usSqw.png?raw=true)\n",
    "\n",
    "\n",
    "### Number of dimensions\n",
    "\n",
    "In general, for a convolutional layers with input dimensions (volume)\n",
    "$W_1 * H_1 * D_1$ and hyperparameters \n",
    "\n",
    "* Number of filters = $K$\n",
    "\n",
    "* Filter sizes = $F$\n",
    "\n",
    "* Stride = $S$\n",
    "\n",
    "* Zero Padding = $P$.\n",
    "\n",
    "We obtain an output volume of $W_2 * H_2 * D_2$ dimensions where  \n",
    "\n",
    "* $W_2 = (W_1 - F + 2P) / S + 1$\n",
    "* $H_2 = (H1 - F + 2P) / S + 1$\n",
    "* $D_2 = K$\n",
    "\n",
    "and the total number of parameters is $(F⋅F⋅D_1)⋅K$ weights and $K$ biases.\n",
    "\n",
    "For an in-depth analysis see [this paper](http://cs231n.github.io/convolutional-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "owIcd9VguyAe"
   },
   "source": [
    "\n",
    "## LeNet\n",
    "CNNs have been developed to model images. They can model images more efficiently (with fewer parameters) than an equivalent MLP. LeNet is a basic CNN for classification. It comes in several versions.\n",
    "\n",
    "We will use a \"LeNet 5\" to classify MNIST digit images:\n",
    "\n",
    "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/lenet5.png?raw=true)\n",
    "\n",
    "\n",
    "To solve a prediction task, the process usually goes as follows (it is the same for MLPs and CNNs):\n",
    "<ol>\n",
    "<li>Determine the network's architecture. This will implicitly determine the number of parameters (weights and biases) of the network.</li>\n",
    "<li>Determine the cost function and the optimization method.</li>\n",
    "<li>Train the weights of the network (i.e., fit the model to train data).</li>\n",
    "<li>Test the network (i.e., evaluate its performance on test data).</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "## Determining the network's architecture\n",
    "### Toolbox\n",
    "**Recall:** To instantiate a particular network in PyTorch, one first subclasses <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Module\">`torch.nn.Module`</a> and then writes the following methods :\n",
    "<ul>\n",
    "<li>The `__init__` method defines the layers. </li>\n",
    "<li>The `forward(input)` method returns the `output`.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "For LeNet 5's '`__init__`, the following classes can be used:\n",
    "<ul>\n",
    "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Conv2d\">`torch.nn.Conv2d(in_channels, out_channels, kernel_size)`</a> applies a 2D convolution on the input channels.</li>\n",
    "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.MaxPool2d\">`torch.nn.MaxPool2d(kernel_size)`</a> applies 2D max pooling on the input channels.</li>\n",
    "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Linear\">`torch.nn.Linear(in_features, out_features)`</a> applies a linear transformation on its input: y = Ax + b.</li>\n",
    "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.ReLU\">`torch.nn.Relu()`</a> applies an elementwise Relu activation: Relu(x) = max(0, x).</li>\n",
    "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Softmax\">`torch.nn.Softmax(dim)`</a> applies a softmax activation to an n-dimensional tensor (normalizes the exponentiated entries).</li>\n",
    "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Sequential\">`torch.nn.sequential`</a> a sequential container in which to add modules in the order in which they will be constructed.</li>\n",
    "</ul>\n",
    "\n",
    "`forward(input)` successively applies the input data to the different layers defined in  `__init__`.\n",
    "\n",
    "Finally, `model.to(\"cuda:0\")` passes the model to an available GPU.\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sR4OQa-4gBAQ"
   },
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        \n",
    "        self.fc = nn.Linear(7*7*32, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block1(x)\n",
    "\n",
    "        out = self.block2(out)\n",
    "        \n",
    "        # Flatten the output of block2\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "model = LeNet5()\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)\n",
    "print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tRo03AR2PFPE"
   },
   "source": [
    "We note that LeNet4 has 28 938 parameters versus 648 010 parameters for an somewhat equivalent MLP with two hidden layers. This reduction in the number of parameters is significant. \n",
    "\n",
    "Here is how we calculate the number of parameters for LeNet5: \n",
    "\n",
    "```\n",
    "1st layer: 16 filters of size 5x5 + 16 biases = 16*5*5 + 16 = 416\n",
    "2nd layer: 16 * 32 filters of size 5x5 + 32 biases = 16*32*5*5 + 32 = 12 832\n",
    "FC layer: 7*7*32*10 + 10 biases = 15 690\n",
    " \n",
    "Total = 416 + 12 832 + 15 690 = 28 938\n",
    "```\n",
    "\n",
    "As a comparison, here is how we calculate the number of parameters of the two hidden layer MLP: \n",
    "The input flattens the 28x28 images into a vector of size 784. The second layer has 500 neurons. Each neuron requires 784 weights + 1 bias. So 500\\*785 parameters. This is then fed to another layer of 500 neurons which adds 501\\*500 parameters. Finally, the output layer has 10 neurons, each with 500 weights and a single bias for a total of 10\\*501 parameters. \n",
    "\n",
    "So in total we have: \n",
    "```\n",
    "500*785 + 501*500 + 10*501 = 648010\n",
    "``` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WuPJFPoDQYLq"
   },
   "outputs": [],
   "source": [
    "# Save the initial weights of model\n",
    "init_model_wts = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9H0ssbh3V1j"
   },
   "source": [
    "## Determine the cost function and the optimization method\n",
    "### Toolbox\n",
    "**Recall:** A common choice for a multi-class task are the following:\n",
    "<ul>\n",
    "<li>**Cost function :** <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss\">`torch.nn.CrossEntropyLoss()`</a>. The cross entropy is often used in this context. It compares a (multivariate) distribution $p$ with a reference distribution $t$. It is minimized for $p=t$ and it is expressed mathematically by: $-\\sum_j t_{ij} \\log(p_{ij})$ where $p$ is the prediction, $t$ the target, $i$ are examples and $j$ the target class.</li>\n",
    "<li>**Optimization method :** <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD(net.parameters(), lr=learning_rate)`</a> a standard stochastic gradient descent (SGD) implementation</li>\n",
    "</ul>\n",
    "\n",
    "### Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uE3Cfrd-hJ0e"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sYEtUb4s3rab"
   },
   "source": [
    "## Train the weights of a network\n",
    "### Toolbox\n",
    "**Recall:** Training a network usually involves iterating for multiple epochs over the training dataset. One epoch corresponds to one pass over the full dataset. \n",
    "\n",
    "The dataset is usually divided into batches. Each epoch will then receive sequentially batches. For each batch we do the following operations:\n",
    "<ol>\n",
    "<li>`optimizer.zero_grad()`: we clear the previously stored gradients.</li>\n",
    "<li>`loss.backward()`: we evaluate the cost, the gradients, and backpropagate the gradients through the computation graph.</li>\n",
    "<li>`optimizer.step()`: we update the parameters using the previously calculated gradients. For SGD, the update is: `weight = weight - learning_rate * gradient`.</li>\n",
    "</ol>\n",
    "\n",
    "### Implementation\n",
    "Just fill in the blanks... Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elxupovwhRSk"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(init_model_wts)\n",
    "\n",
    "since = time.time()\n",
    "\n",
    "num_epochs = 10\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "print(\"# Start training #\")\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_n_iter = 0\n",
    "    \n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over train data\n",
    "    for images, labels in train_loader:  \n",
    "\n",
    "        # put images on proper device (GPU)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the gradient buffer\n",
    "        optimizer.zero_grad()          \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        train_n_iter += 1\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_n_iter = 0\n",
    "    \n",
    "    # Set model to evaluate mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Iterate over valid data\n",
    "    for images, labels in valid_loader:  \n",
    "        \n",
    "        # put images on proper device (GPU)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "       \n",
    "        # Statistics\n",
    "        valid_loss += loss.item()\n",
    "        valid_n_iter += 1\n",
    "    \n",
    "    train_loss_history.append(train_loss / train_n_iter)\n",
    "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
    "    \n",
    "    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n",
    "    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n",
    "    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "\n",
    "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3fozzXmdRGTs"
   },
   "source": [
    "Let's plot the training curves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TUGbFeg5RHFZ"
   },
   "outputs": [],
   "source": [
    "# Save history for later\n",
    "lenet5_train_loss_history = train_loss_history\n",
    "lenet5_valid_loss_history = valid_loss_history\n",
    "\n",
    "# Plot training and validation curve\n",
    "x = range(1, num_epochs + 1)\n",
    "plt.plot(x, lenet5_train_loss_history, label='train')\n",
    "plt.plot(x, lenet5_valid_loss_history, label='valid')\n",
    "\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H6kWtULrSDXL"
   },
   "source": [
    "We can overlay the validation curves on top of the training curves for training of LeNet5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rE1qsmvaSTjH"
   },
   "outputs": [],
   "source": [
    "# Plot training and validation curve\n",
    "x = range(1, num_epochs + 1)\n",
    "plt.plot(x, mlp_train_loss_history, label='MLP train')\n",
    "plt.plot(x, mlp_valid_loss_history, label='MLP valid')\n",
    "plt.plot(x, lenet5_train_loss_history, label='LeNet5 train')\n",
    "plt.plot(x, lenet5_valid_loss_history, label='LeNet5 valid')\n",
    "\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tq79RTld3xyc"
   },
   "source": [
    "## Test the network\n",
    "### Toolbox\n",
    "**Recall:** we evaluate the network's performance on test data.\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "da47-MilhpN7"
   },
   "outputs": [],
   "source": [
    "# Set model to evaluate mode\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Iterate over data.\n",
    "for images, labels in test_loader:\n",
    "    \n",
    "    # put images on proper device (GPU)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # No need to flatten the images here !\n",
    "    \n",
    "\n",
    "    # Forward\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "    # Statistics\n",
    "    total += labels.size(0)\n",
    "    correct += torch.sum(predicted == labels.data)\n",
    "\n",
    "print('Accuracy on the test set: {:.2f}%'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "voN-_iO_RQ8A"
   },
   "source": [
    "The best results are obtained after 10 epochs!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFj_W39u5voa"
   },
   "source": [
    "## Methods for improving training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "chov57bzu76J"
   },
   "source": [
    "### Batch normalization\n",
    "Batch normalization is a trick that often yields faster training. It acts as a regularizer by normalizing the inputs (by batch). Further, the operation is differentiable.\n",
    "\n",
    "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/1_Hiq-rLFGDpESpr8QNsJ1jg.png?raw=true)\n",
    "\n",
    "\n",
    "For additional information see [article](https://arxiv.org/pdf/1502.03167v3.pdf).\n",
    "\n",
    "### Toolbox\n",
    "Batch normalization is already implemented for us. To add it to our LeNet5 network, just add another layer in `__init__`. The following class can be used:\n",
    "<ul>\n",
    "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.BatchNorm2d\">`nn.BatchNorm2d(num_features)`</a>: add batch normalisation to a 4-dimensional input encoded in a 3-dimensional tensor.</li>\n",
    "</ul>\n",
    "\n",
    "### Implement\n",
    "Below, you need to add batch normalization to the original LeNet5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6pi_mhvg8E4E"
   },
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        \n",
    "        self.fc = nn.Linear(7*7*32, 10)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        out = self.block1(x)\n",
    "\n",
    "        out = self.block2(out)\n",
    "        \n",
    "        # Flatten the output of block2\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "               \n",
    "        return out\n",
    "        \n",
    "model = LeNet5()\n",
    "model = model.to(device)\n",
    "  \n",
    "print(model)\n",
    "\n",
    "print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "271HrQBNccH1"
   },
   "source": [
    "Note that batch normalization adds parameters. Our new LeNet5 with batch normalization has 29 034 parameters (versus 28 938 for the original LeNet5 model without batch normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnbesUOudO_s"
   },
   "outputs": [],
   "source": [
    "# Save the initial weights of model\n",
    "init_model_wts = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rj4R3qV5ABFC"
   },
   "source": [
    "**The rest (i.e., the cost function, the optimizer, the training loops, and the testing procedures) remain unchanged!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JgEcoSRQAVtA"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.load_state_dict(init_model_wts)\n",
    "\n",
    "since = time.time()\n",
    "\n",
    "num_epochs = 10\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "print(\"# Start training #\")\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_n_iter = 0\n",
    "    \n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over train data\n",
    "    for images, labels in train_loader:  \n",
    "\n",
    "        # put images on proper device (GPU)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the gradient buffer\n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        train_n_iter += 1\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_n_iter = 0\n",
    "    \n",
    "    # Set model to evaluate mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Iterate over valid data\n",
    "    for images, labels in valid_loader:  \n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "      \n",
    "        # Statistics\n",
    "        valid_loss += loss.item()\n",
    "        valid_n_iter += 1\n",
    "    \n",
    "    train_loss_history.append(train_loss / train_n_iter)\n",
    "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
    "    \n",
    "    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n",
    "    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n",
    "    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "\n",
    "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6pxvS_yUeWog"
   },
   "source": [
    "We obtain even better results after 10 epochs!\n",
    "\n",
    "Let's have a look at the training and validation curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hrv-o7Cle0ty"
   },
   "outputs": [],
   "source": [
    "# Save history for later\n",
    "lenet5_batchnorm_train_loss_history = train_loss_history\n",
    "lenet5_batchnorm_valid_loss_history = valid_loss_history\n",
    "\n",
    "# Plot training and validation curve\n",
    "x = range(1, num_epochs + 1)\n",
    "plt.plot(x, lenet5_train_loss_history, label='LeNet5 train')\n",
    "plt.plot(x, lenet5_valid_loss_history, label='LeNet5 valid')\n",
    "plt.plot(x, lenet5_batchnorm_train_loss_history, label='LeNet5 batch norm train')\n",
    "plt.plot(x, lenet5_batchnorm_valid_loss_history, label='LeNet5 batch norm valid')\n",
    "\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CHqHLCP6Crmp"
   },
   "source": [
    "# Transfer Learning : finetuning a CNN\n",
    "**Attribution:** this part of the tutorial is from: http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. This is referred to as transfer learning.\n",
    "\n",
    "We will now explore the finetuning of a convolutionational network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itOkLFisvKqI"
   },
   "source": [
    "## Downloading the data and creating the data loader\n",
    "\n",
    "We will use torchvision and torch.utils.data packages for loading the data.\n",
    "\n",
    "The problem we’re going to solve today is to train a model to classify ants and bees. We have about 120 training images each for ants and bees. There are 75 validation images for each class. Usually, this is a very small dataset to generalize upon, if trained from scratch. Since we are using transfer learning, we should be able to generalize reasonably well.\n",
    "\n",
    "This dataset is a very small subset of imagenet.\n",
    "\n",
    "Here are a few example images from our training dataset:\n",
    "\n",
    "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/fourmi_abeille.png?raw=true)\n",
    "\n",
    "### Toolbox\n",
    "**Recall:** an easy way to load the data in PyTorch involves: \n",
    "<ol>\n",
    "<li>Sublcass<a href=\"http://pytorch.org/docs/master/data.html#torch.utils.data.Dataset\">`torch.utils.data.Dataset`</a> and write the `__getitem__` and `__len__` methods. (Note that these do not load the data in memory.)</li>\n",
    "<li>Use<a href=\"http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader\">`torch.utils.data.DataLoader`</a> to read and load the data into memory.</li>\n",
    "</ol>\n",
    "\n",
    "**Note:** <a href=\"http://pytorch.org/docs/master/torchvision/datasets.html#torchvision-datasets\">`torchvision.datasets`</a> provides an alternative for loading data from a directory.\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LW-1CauxEQM"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "## DOWNLOAD DATASET ##\n",
    "if [ ! -d \"hymenoptera_data\" ]; then\n",
    "  wget --quiet https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
    "  unzip -q hymenoptera_data.zip\n",
    "  rm hymenoptera_data.zip\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMvya9Oxps5z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "def make_dataset(root, split_type):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_dir : string\n",
    "    Directory with all the images.\n",
    "    split_type : string\n",
    "    The name of the split in {'train', 'valid'}.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    images : dict\n",
    "    Dict of images path for each classes for a specific split type.\n",
    "    \"\"\"\n",
    "\n",
    "    images = {}\n",
    "    root = os.path.join(root, split_type)\n",
    "\n",
    "    for classes in sorted(os.listdir(root)):\n",
    "        images[classes] = []\n",
    "        path_classes = os.path.join(root, classes)\n",
    "\n",
    "        for root_, _, fnames in sorted(os.walk(path_classes)):\n",
    "            for fname in sorted(fnames):\n",
    "                if fname.endswith('.jpg'):\n",
    "                    item = os.path.join(root_, fname)\n",
    "                    images[classes].append(item)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "class HymenopteraDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Hymenoptera dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, split_type='train', transform=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        root_dir : string\n",
    "           Directory with all the images.\n",
    "        split_type : string\n",
    "           The name of the split in {'train', 'valid', 'test', 'train_valid'}.\n",
    "        transform : callable, optional\n",
    "           Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split_type = split_type\n",
    "        self.transform = transform\n",
    "        self.classes = {'ants': 0, 'bees': 1}\n",
    "\n",
    "        imgs_ = []\n",
    "        target_ = []\n",
    "\n",
    "        if split_type == 'train':\n",
    "            imgs = make_dataset(root_dir, 'train')\n",
    "            for k, v in imgs.items():\n",
    "                imgs_ += imgs[k][:int(0.8*len(v))]\n",
    "                target_ += [self.classes[k]] * len(imgs_)\n",
    "\n",
    "        elif split_type == 'valid':\n",
    "            imgs = make_dataset(root_dir, 'train')\n",
    "            for k, v in imgs.items():\n",
    "                imgs_ += imgs[k][int(0.8*len(v)):]\n",
    "                target_ += [self.classes[k]] * len(imgs_)\n",
    "\n",
    "        elif split_type == 'train_valid':\n",
    "            imgs = make_dataset(root_dir, 'train')\n",
    "            for k, v in imgs.items():\n",
    "                imgs_ += imgs[k]\n",
    "                target_ += [self.classes[k]] * len(imgs_)\n",
    "\n",
    "        elif split_type == 'test':\n",
    "            imgs = make_dataset(root_dir, 'val')\n",
    "            for k, v in imgs.items():\n",
    "                imgs_ += imgs[k]\n",
    "                target_ += [self.classes[k]] * len(imgs_)\n",
    "\n",
    "        self.imgs = imgs_\n",
    "        self.target = np.array(target_)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the number of image in the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "           The number of images in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get the items : image, target\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "           Index\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        img : tensor\n",
    "           The image.\n",
    "        target : int\n",
    "           Target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path = self.imgs[index]\n",
    "        target = self.target[index]\n",
    "\n",
    "        with open(path, 'rb') as f:\n",
    "            with Image.open(f) as img:\n",
    "                img.convert('RGB')\n",
    "\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c9BHo5WsOghr"
   },
   "source": [
    "## Data augmentation\n",
    "\n",
    "Data augmentation is a trick which can be used to augment the effective size of your training data. This can, in turn, help reduce overfitting. It consists in creating new data by transforming available training data. For example for images, we can resize, stretch, rotate, mirror them to obtain additional images. Here are some examples of image augmentations (right) from an original image (left): \n",
    "\n",
    "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/1_Jujct_Pt-zvdWtSFpHUp3Q.png?raw=true)\n",
    "\n",
    "\n",
    "To augment your data, <a href=\"http://pytorch.org/docs/master/torchvision/transforms.html#torchvision-transforms\">`torchvision.transforms`</a> \n",
    "provides common image transformations. These transformations can be applied successively by using<a href=\"http://pytorch.org/docs/master/torchvision/transforms.html#torchvision.transforms.Compose\">`torchvision.transforms.Compose`</a>.\n",
    "\n",
    "Add the following transformations using your training data:\n",
    "* Random crop to resize each image to size 224x224\n",
    "* Some probability of taking a mirror of the image \n",
    "* An image normalization with the following means and standard deviations: [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "For the validation data, resize the image to have size 256x256, take a crop starting from the center and normalize the image with the same values as used for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v94wZLuOp4m3"
   },
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i9eB9I_p5sJ"
   },
   "outputs": [],
   "source": [
    "# Dataset loader\n",
    "data_dir = 'hymenoptera_data'\n",
    "\n",
    "data_train = HymenopteraDataset(data_dir, 'train', data_transforms['train'])\n",
    "train_loader = DataLoader(data_train, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "data_valid = HymenopteraDataset(data_dir, 'valid', data_transforms['valid'])\n",
    "valid_loader = DataLoader(data_valid, batch_size=4, shuffle=False, num_workers=4)\n",
    "\n",
    "data_test = HymenopteraDataset(data_dir, 'test', data_transforms['valid'])\n",
    "test_loader = DataLoader(data_test, batch_size=4, shuffle=False, num_workers=4)\n",
    "\n",
    "print('# images in data train: {}'.format(len(data_train)))\n",
    "print('# images in data valid: {}'.format(len(data_valid)))\n",
    "print('# images in data test: {}'.format(len(data_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pl1X1vt5AVaf"
   },
   "source": [
    "Let's have a look at our training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f4kbOO5XAUO5"
   },
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(train_loader))\n",
    "\n",
    "print('Classes: {}'.format(data_train.classes))\n",
    "print('Inputs size: {}'.format(inputs.size()))\n",
    "print('Classes size: {}'.format(classes.size()))\n",
    "\n",
    "# Random image of the batch\n",
    "idx = np.random.randint(len(inputs))\n",
    "img = inputs[idx]\n",
    "labels = list(data_train.classes.keys())\n",
    "img_label = labels[(classes[idx])]\n",
    "\n",
    "img = img.numpy().transpose((1, 2, 0))\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "img = std * img + mean\n",
    "img = np.clip(img, 0, 1)\n",
    "plt.imshow(img)\n",
    "plt.title(img_label)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qh3bKAO7Lf14"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def imshow(img, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "inputs, classes = next(iter(train_loader))\n",
    "\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "class_names = data_train.classes\n",
    "class_names = {class_names[k]: k for k in class_names.keys()}\n",
    "\n",
    "imshow(out, title=[class_names[int(x)] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gN9JbYASp4tE"
   },
   "source": [
    "## Determing the network's architecture\n",
    "### Toolbox\n",
    "\n",
    "We will reuse a network pre-trained on ImageNet. To do so, we first load the pre-trained model and reinitialize its final layer (the fully connected one). Luckily, in PyTorch <a href=\"http://pytorch.org/docs/0.1.12/torchvision/models.html#module-torchvision.models\">`torchvision.models`</a>\n",
    "proposes pre-trained models on ImageNet. \n",
    "\n",
    "A common choice for classification problem (our setting) is to use the *ResNet18* model. For more information regarding this model, see [article](https://arxiv.org/abs/1512.03385). Useful information is also provided in the PyTorch documentation of the model:\n",
    "<a href=\"http://pytorch.org/docs/0.1.12/torchvision/models.html#torchvision.models.resnet18\">`torchvision.models.resnet18(pretrained=True)`</a>\n",
    "\n",
    "Here is an example of a residual bloc (residual blocks are the core of residual networks such as *ResNet18*).\n",
    "\n",
    "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/bloc_residuel.png?raw=true)\n",
    "\n",
    "\n",
    "**Recall:** <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Linear\">`torch.nn.Linear(in_features, out_features)`</a> performs a linear transformation to input data x: y = Ax + b.\n",
    "\n",
    "### Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TiQfdQxiMJPQ"
   },
   "source": [
    "To gain insights into the value of pre-trained networks, we will first train a network from scratch and then compare it to a pre-trained version of the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nxBgVe4NISea"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# Load non-pre-trained resnet18 model\n",
    "model = models.resnet18(pretrained=False)\n",
    "\n",
    "# Reset last layer\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQTyN1tkZyYG"
   },
   "outputs": [],
   "source": [
    "# Save the initial weights of model\n",
    "init_model_wts = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_ut6uP0qpWW"
   },
   "source": [
    "## Determine the cost function and the optimization method\n",
    "### Toolbox\n",
    "**Recall:** A common choice for a multi-class task are the following:\n",
    "<ul>\n",
    "<li>**Cost function :** <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss\">`torch.nn.CrossEntropyLoss()`</a>. The cross entropy is often used in this context. It compares a (multivariate) distribution $p$ with a reference distribution $t$. It is minimized for $p=t$ and it is expressed mathematically by: $-\\sum_j t_{ij} \\log(p_{ij})$ where $p$ is the prediction,, $t$ the target, $i$ are examples and $j$ the target class.</li>\n",
    "<li>**Optimization method :** <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD(net.parameters(), lr=learning_rate)`</a> a standard stochastic gradient descent (SGD) implementation. Here use a learning rate of $1e-3$ and a momentum of $0.9$</li>\n",
    "</ul>\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JW00K1Ssqqcm"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9uhoTb410Utx"
   },
   "source": [
    "## Training a network\n",
    "### Toolbox\n",
    "\n",
    "**Recall:** Training a network usually involves iterating for multiple epochs over the training dataset. One epoch corresponds to one pass over the full dataset. The dataset is usually divided into batches. Each epoch will then receive sequentially batches. For each batch we do the following operations:\n",
    "<ol>\n",
    "<li>`optimizer.zero_grad()`: we clear the previously stored gradients.</li>\n",
    "<li>`loss.backward()`: we evaluate the cost, the gradients and backpropagate the gradients through the computation graph.</li>\n",
    "<li>`optimizer.step()`: we update the parameters using the previously calculated gradients. For SGD, the update is: `weight = weight - learning_rate * gradient`. Adam is similar but also adds bells and whistles (e.g., it adapts the learning rate over time for each parameter)</li>\n",
    "</ol>\n",
    "\n",
    "**Bonus:** To train deep neural networks we often use these additional tricks:\n",
    "<ul>\n",
    "<li>early stopping: monitors the validation error of the model and stops training if it begins to overfit (e.g., if it stops improving).</li>\n",
    "<li>checkpointing: To do so, it is common to save the network's weight (you can obtain them using `model.state_dict()`) throughout training.</li>\n",
    "<li>printing execution time. To do so, you can use `time.time()`.</li>\n",
    "</ul>\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hXNpyIj_wxRP"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "since = time.time()\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "num_epochs = 25\n",
    "best_acc = 0.0\n",
    "\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "print(\"# Start training #\")\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_n_iter = 0\n",
    "    \n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over train data\n",
    "    for images, labels in train_loader:  \n",
    "        \n",
    "        # put images on proper device (GPU)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the gradient buffer\n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        train_n_iter += 1\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_n_iter = 0\n",
    "    \n",
    "    # Set model to evaluate mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Iterate over valid data\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for images, labels in valid_loader:  \n",
    "        \n",
    "        # put images on proper device (GPU)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "        # Statistics\n",
    "        total += labels.size(0)\n",
    "        correct += torch.sum(predicted == labels.data)\n",
    "        valid_loss += loss.item()\n",
    "        valid_n_iter += 1\n",
    "    \n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    # Deep copy the best model\n",
    "    if epoch_acc > best_acc:\n",
    "        best_acc = epoch_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    train_loss_history.append(train_loss / train_n_iter)\n",
    "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
    "    \n",
    "    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n",
    "    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n",
    "    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "\n",
    "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "print('\\n\\nBest valid accuracy: {:.2f}'.format(best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xr8-PgSmiQSF"
   },
   "source": [
    "Let's visualize the training and validation curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njb7oaT-hfk3"
   },
   "outputs": [],
   "source": [
    "resnet18_train_loss_history = train_loss_history\n",
    "resnet18_valid_loss_history = valid_loss_history\n",
    "\n",
    "# Plot training and validation curve\n",
    "x = range(1, num_epochs + 1)\n",
    "plt.plot(x, resnet18_train_loss_history, label='ResNet18 train')\n",
    "plt.plot(x, resnet18_valid_loss_history, label='ResNet18 valid')\n",
    "\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vcqa0H5p-FHq"
   },
   "source": [
    "## Testing the network\n",
    "### Toolbox\n",
    "**Recall:** we evaluate the network's performance on test data.\n",
    "\n",
    "**Note:** Here, we do not have a test set so we will use the validation set instead (DO NOT DO THIS IN PRACTICE).\n",
    "\n",
    "**Using the best model:** we want to reuse the weights from the best model to evaluate it on a new dataset (here validation data). These weights have been saved in the training phase in `best_model_wts`. To load them, you can use `model.load_state_dict(best_model_wts)`.\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C1DCCblW_EPo"
   },
   "outputs": [],
   "source": [
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Set model to evaluate mode\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Iterate over test data\n",
    "for images, labels in test_loader:\n",
    "    \n",
    "    # put images on proper device (GPU)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Forward\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # Statistics\n",
    "    total += labels.size(0)\n",
    "    correct += torch.sum(predicted == labels.data)\n",
    "\n",
    "print('Accuracy on the test set: {:.2f}%'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d38Q32VMN0Hb"
   },
   "source": [
    "With pre-trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qMbvwaR5Ny-K"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# Load pre-trained resnet18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Reset last layer\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XAcPL09nN-Ki"
   },
   "outputs": [],
   "source": [
    "# Save the initial weights of model\n",
    "init_model_wts = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88Nhc4mlN-sc"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U1pQBFbAOEjd"
   },
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "num_epochs = 25\n",
    "best_acc = 0.0\n",
    "\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "print(\"# Start training #\")\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_n_iter = 0\n",
    "    \n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over train data\n",
    "    for images, labels in train_loader:  \n",
    "        \n",
    "        # put images on proper device (GPU)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the gradient buffer\n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        train_n_iter += 1\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_n_iter = 0\n",
    "    \n",
    "    # Set model to evaluate mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Iterate over valid data\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for images, labels in valid_loader:  \n",
    "        \n",
    "        # put images on proper device (GPU)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "        # Statistics\n",
    "        total += labels.size(0)\n",
    "        correct += torch.sum(predicted == labels.data)\n",
    "        valid_loss += loss.item()\n",
    "        valid_n_iter += 1\n",
    "    \n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    # Deep copy the best model\n",
    "    if epoch_acc > best_acc:\n",
    "        best_acc = epoch_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    train_loss_history.append(train_loss / train_n_iter)\n",
    "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
    "    \n",
    "    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n",
    "    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n",
    "    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "\n",
    "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "print('\\n\\nBest valid accuracy: {:.2f}'.format(best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d_x_Kot7iEnI"
   },
   "source": [
    "Let's have a look at train and validation error curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVUPxQCehyzc"
   },
   "outputs": [],
   "source": [
    "resnet18_pretrained_train_loss_history = train_loss_history\n",
    "resnet18_pretrained_valid_loss_history = valid_loss_history\n",
    "\n",
    "# Plot training and validation curve\n",
    "x = range(1, num_epochs + 1)\n",
    "plt.plot(x, resnet18_train_loss_history, label='ResNet18 train')\n",
    "plt.plot(x, resnet18_valid_loss_history, label='ResNet18 valid')\n",
    "plt.plot(\n",
    "      x, resnet18_pretrained_train_loss_history,\n",
    "    label='ResNet18 pretrained train')\n",
    "plt.plot(\n",
    "      x, resnet18_pretrained_valid_loss_history,\n",
    "    label='ResNet18 pretrained valid')\n",
    "\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "keq8hL3yiCOG"
   },
   "source": [
    "Let's test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJy4FdfQfc7z"
   },
   "outputs": [],
   "source": [
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Set model to evaluate mode\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Iterate over test data\n",
    "for images, labels in test_loader:\n",
    "    \n",
    "    # put images on proper device (GPU)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "        \n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # Statistics\n",
    "    total += labels.size(0)\n",
    "    correct += torch.sum(predicted == labels.data)\n",
    "\n",
    "print('Accuracy on the test set: {:.2f}%'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMd3mx5KKYXk"
   },
   "source": [
    "Note that we obtain a better accuracy on the test versus the model with the weights that hadn't been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u69CX2svt_4l"
   },
   "source": [
    "# Follow-up (optional)\n",
    "\n",
    "If you wish to know more about convolutional neural networks here are a few pointers to more advanced tasks that can be performed with these models:\n",
    "\n",
    "\n",
    "## Image segmentation\n",
    "\n",
    "While image classification is useful, it can be even more useful to determine where a particular object of interest is in the image. This is called image segmentation (it is trying to segment the image into various regions). A popular algorithm for image segmentation is [Mask R-CNN](https://arxiv.org/abs/1703.06870). Here is a tutorial that goes over the implementation of this algorithm [here].(https://github.com/matterport/Mask_RCNN/blob/master/samples/shapes/train_shapes.ipynb)\n",
    "\n",
    "Here is an example of a segmented image: \n",
    "\n",
    "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/detection_final.png?raw=true)\n",
    "\n",
    "\n",
    "## Generative models (GAN example)\n",
    "\n",
    "A generative adversarial network (GAN) consists of two networks that are \"playing\" against one another. Each is trying to fool the other. The first, the generator, tries to generate realistic data (e.g., images) while the second, the discriminator, tries to determine whether the generated data is real or fake. \n",
    "\n",
    "Training both of these networks together can lead to the generation of high-quality data (often images). \n",
    "For example, here is an MNIST-list digit generated from a GAN trained model:\n",
    "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/1_nAVqFluPijpBWR2tI4gCxg.png?raw=true)\n",
    "\n",
    "More information is available in the original [GAN paper](http://papers.nips.cc/paper/5423-generative-adversarial-nets). [Here](https://github.com/diegoalejogm/gans) is an implementation of it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IsLwCjnbWkk4"
   },
   "source": [
    "# References\n",
    "Various parts of this tutorial are inspired from these other tutorials: \n",
    "<ul>\n",
    "<li>https://github.com/andrewliao11/dni.pytorch/blob/master/mlp.py\n",
    "<li>https://github.com/andrewliao11/dni.pytorch/blob/master/cnn.py\n",
    "<li>http://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
    "<li>http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    " <li>http://cs231n.github.io/convolutional-networks/\n",
    " <li>http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html#convolution-as-a-matrix-operation\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_solutions.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
>>>>>>> upstream/master
