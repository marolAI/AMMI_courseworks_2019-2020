{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd: automatic differentiation\n",
    "\n",
    "The ``autograd`` package provides automatic differentiation for all operations\n",
    "on Tensors. It is a define-by-run framework, which means that your backprop is\n",
    "defined by how your code is run, and that every single iteration can be\n",
    "different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[1., 2.],\n        [3., 4.]], requires_grad=True)\n"
    }
   ],
   "source": [
    "# Create a 2x2 tensor with gradient-accumulation capabilities\n",
    "x = torch.tensor([[1, 2], [3, 4]], requires_grad=True, dtype=torch.float32)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do an operation on the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[-1.,  0.],\n        [ 1.,  2.]], grad_fn=<SubBackward0>)\n"
    }
   ],
   "source": [
    "# Deduct 2 from all elements\n",
    "y = x - 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``y`` was created as a result of an operation, so it has a ``grad_fn``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<SubBackward0 object at 0x7fb6f04877f0>\n"
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "None\n"
    }
   ],
   "source": [
    "# What's happening here?\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<SubBackward0 at 0x7fb6d804e5c0>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's dig further...\n",
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<AccumulateGrad at 0x7fb6d804ec18>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 2.],\n        [3., 4.]], requires_grad=True)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0][0].variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[ 3.,  0.],\n        [ 3., 12.]], grad_fn=<MulBackward0>)\ntensor(4.5000, grad_fn=<MeanBackward0>)\n"
    }
   ],
   "source": [
    "# Do more operations on y\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Collecting torchviz\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n\u001b[K    100% |████████████████████████████████| 51kB 328kB/s \n\u001b[?25hRequirement already satisfied: torch in /home/andria/.local/lib/python3.7/site-packages (from torchviz) (1.3.1)\nCollecting graphviz (from torchviz)\n  Downloading https://files.pythonhosted.org/packages/f5/74/dbed754c0abd63768d3a7a7b472da35b08ac442cf87d73d5850a6f32391e/graphviz-0.13.2-py2.py3-none-any.whl\nRequirement already satisfied: numpy in /usr/lib/python3/dist-packages (from torch->torchviz) (1.16.2)\nBuilding wheels for collected packages: torchviz\n  Running setup.py bdist_wheel for torchviz ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/andria/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\nSuccessfully built torchviz\nInstalling collected packages: graphviz, torchviz\nSuccessfully installed graphviz-0.13.2 torchviz-0.0.1\n"
    }
   ],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualise the computational graph! (thks @szagoruyko)\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"99pt\" height=\"214pt\"\n viewBox=\"0.00 0.00 99.00 214.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 210)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-210 95,-210 95,4 -4,4\"/>\n<!-- 140423784191704 -->\n<g id=\"node1\" class=\"node\">\n<title>140423784191704</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"91,-21 0,-21 0,0 91,0 91,-21\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140423284731408 -->\n<g id=\"node2\" class=\"node\">\n<title>140423284731408</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"91,-78 0,-78 0,-57 91,-57 91,-78\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140423284731408&#45;&gt;140423784191704 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140423284731408&#45;&gt;140423784191704</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M45.5,-56.7787C45.5,-49.6134 45.5,-39.9517 45.5,-31.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"49.0001,-31.1732 45.5,-21.1732 42.0001,-31.1732 49.0001,-31.1732\"/>\n</g>\n<!-- 140423284975040 -->\n<g id=\"node3\" class=\"node\">\n<title>140423284975040</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"90.5,-135 .5,-135 .5,-114 90.5,-114 90.5,-135\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SubBackward0</text>\n</g>\n<!-- 140423284975040&#45;&gt;140423284731408 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140423284975040&#45;&gt;140423284731408</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M40.0885,-113.7787C38.8317,-106.6134 38.4599,-96.9517 38.9733,-88.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"42.4738,-88.498 40.097,-78.1732 35.5164,-87.7267 42.4738,-88.498\"/>\n</g>\n<!-- 140423284975040&#45;&gt;140423284731408 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140423284975040&#45;&gt;140423284731408</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.9115,-113.7787C52.1683,-106.6134 52.5401,-96.9517 52.0267,-88.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"55.4836,-87.7267 50.903,-78.1732 48.5262,-88.498 55.4836,-87.7267\"/>\n</g>\n<!-- 140423284976664 -->\n<g id=\"node4\" class=\"node\">\n<title>140423284976664</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"72.5,-206 18.5,-206 18.5,-171 72.5,-171 72.5,-206\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (2, 2)</text>\n</g>\n<!-- 140423284976664&#45;&gt;140423284975040 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140423284976664&#45;&gt;140423284975040</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M45.5,-170.6724C45.5,-162.8405 45.5,-153.5893 45.5,-145.4323\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"49.0001,-145.2234 45.5,-135.2234 42.0001,-145.2235 49.0001,-145.2234\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": "<graphviz.dot.Digraph at 0x7fb6f5c655c0>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "Let's backprop now `out.backward()` is equivalent to doing `out.backward(torch.tensor([1.0]))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print gradients $\\frac{\\text{d}o}{\\text{d}x}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[-1.5000,  0.0000],\n        [ 1.5000,  3.0000]])\n"
    }
   ],
   "source": [
    "# Compute it by hand BEFORE executing this\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do many crazy things with autograd!\n",
    "> With Great *Flexibility* Comes Great Responsibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([-1061.9554,   477.7362,  -384.6196], grad_fn=<MulBackward0>)\n"
    }
   ],
   "source": [
    "# Dynamic graphs!\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "i = 0\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    i += 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
    }
   ],
   "source": [
    "# If we don't run backward on a scalar we need to specify the grad_output\n",
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "8\n"
    }
   ],
   "source": [
    "# BEFORE executing this, can you tell what would you expect it to print?\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable decides the tensor's range below\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([1., 1., 1.])\ntensor([1., 2., 3.])\n"
    }
   ],
   "source": [
    "# Both x and w that allows gradient accumulation\n",
    "x = torch.arange(1., n + 1, requires_grad=True)\n",
    "w = torch.ones(n, requires_grad=True)\n",
    "z = w @ x\n",
    "z.backward()\n",
    "print(x.grad, w.grad, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "None\ntensor([1., 2., 3.])\n"
    }
   ],
   "source": [
    "# Only w that allows gradient accumulation\n",
    "x = torch.arange(1., n + 1)\n",
    "w = torch.ones(n, requires_grad=True)\n",
    "z = w @ x\n",
    "z.backward()\n",
    "print(x.grad, w.grad, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "element 0 of tensors does not require grad and does not have a grad_fn\n"
    }
   ],
   "source": [
    "x = torch.arange(1., n + 1)\n",
    "w = torch.ones(n, requires_grad=True)\n",
    "\n",
    "# Regardless of what you do in this context, all torch tensors will not have gradient accumulation\n",
    "with torch.no_grad():\n",
    "    z = w @ x\n",
    "\n",
    "try:\n",
    "    z.backward()  # PyTorch will throw an error here, since z has no grad accum.\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More stuff\n",
    "\n",
    "Documentation of the automatic differentiation package is at\n",
    "http://pytorch.org/docs/autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}