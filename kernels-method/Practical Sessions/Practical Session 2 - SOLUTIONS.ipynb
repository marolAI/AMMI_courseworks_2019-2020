{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Session 2\n",
    "### Kernel Methods for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by Yunlong Jiao / Romain Menegaux, 19 May 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.14 |Anaconda, Inc.| (default, Oct  5 2017, 02:28:52) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model as lm\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement (naive) solvers to Ridge Regression, Weighted Ridge Regression and Logistic Ridge Regression (using Iteratively Reweighted Least Squares). See notes for the mathematical derivation.\n",
    "2. Simulate some toy data to check if our solvers give correct solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Toy data\n",
    "np.random.seed(42) # for reproducibility\n",
    "n = 100 # number of samples\n",
    "p = 10 # number of features\n",
    "X = np.random.normal(0, 1, (n, p))\n",
    "X = sklearn.preprocessing.scale(X) # scale to 0 mean and 1 standard deviation\n",
    "beta_star = np.random.normal(0, 1, p)\n",
    "y = X.dot(beta_star) + 0.2 * np.random.normal(0, 1, n) # Xbeta + noise\n",
    "\n",
    "def compare(beta1, beta2):\n",
    "    print('''\n",
    "Our solver:\n",
    "{}\n",
    "Scikit-learn:\n",
    "{}\n",
    "\n",
    "Difference between the two:\n",
    "{}\n",
    "        '''.format(beta1, beta2, np.sum((beta1-beta2)**2))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ridge Regression (RR)\n",
    "\n",
    "Given $X \\in \\mathbb{R}^{n \\times p}$ and $y \\in \\mathbb{R}^n$, solve\n",
    "$$\n",
    "\\min_{\\beta \\in \\mathbb{R}^p} \\frac{1}{n} \\|y - X \\beta\\|^2 + \\lambda \\|\\beta\\|^2 \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient (Jacobian) of the objective is:\n",
    "$$\\nabla f(\\beta) = \\frac{2}{n}X^\\top(X\\beta - y) + 2\\lambda \\beta$$\n",
    "\n",
    "The optimal solution verifies:\n",
    "<font color='green'>\n",
    "$$\\nabla f(\\beta) = 0 \\iff (X^\\top X + n\\lambda I)\\beta = X^\\top y$$\n",
    "</font>\n",
    "\n",
    "$\\|y - X \\beta\\|^2 = (X \\beta - y)^\\top(X \\beta - y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ridge Regression (RR)\n",
    "def solveRR(y, X, lam):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == n)\n",
    "    \n",
    "    A = X.T.dot(X)\n",
    "    # Adjust diagonal due to Ridge\n",
    "    A[np.diag_indices_from(A)] += lam * n\n",
    "    b = X.T.dot(y)\n",
    "    \n",
    "    # Hint:\n",
    "    beta = np.linalg.solve(A, b)\n",
    "    # Finds solution to the linear system Ax = b\n",
    "    return (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it out:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our solver:\n",
      "[ 1.27929172  0.78935356  0.05064497 -0.55474398  0.65276533  0.32637554\n",
      "  0.765293    0.63326617  0.97285396 -0.5294559 ]\n",
      "Scikit-learn:\n",
      "[ 1.27929172  0.78935356  0.05064497 -0.55474398  0.65276533  0.32637554\n",
      "  0.765293    0.63326617  0.97285396 -0.5294559 ]\n",
      "\n",
      "Difference between the two:\n",
      "8.6474254503e-32\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "lam = 0.1\n",
    "\n",
    "# Our solver\n",
    "beta1 = solveRR(y, X, lam)\n",
    "\n",
    "# Python solver\n",
    "alpha = lam * X.shape[0]\n",
    "model = lm.Ridge(alpha=alpha, fit_intercept=False, normalize=False)\n",
    "beta2 = model.fit(X, y).coef_\n",
    "\n",
    "# Check\n",
    "compare(beta1, beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Weighted Ridge Regression (WRR)\n",
    "\n",
    "Given $X \\in \\mathbb{R}^{n \\times p}$ and $y \\in \\mathbb{R}^n$, and weights $w \\in \\mathbb{R}^n_+$, solve\n",
    "$$\n",
    "\\min_{\\beta \\in \\mathbb{R}^p} \\frac{1}{n} \\sum_{i=1}^n w_i (y_i - \\beta^\\top x_i)^2 + \\lambda \\|\\beta\\|^2 \\,.\n",
    "$$\n",
    "\n",
    "You can think of $w_i$ as importance or confidence you have in point $i$\n",
    "$$$$\n",
    "\n",
    "Since the weights $w_i$ are non-negative, we can pull $w_i$ inside the parenthesis:\n",
    "$$\\sum_{i=1}^n w_i (y_i - \\beta^\\top x_i)^2 = \\sum_{i=1}^n  (\\sqrt{w_i}y_i - \\beta^\\top \\sqrt{w_i} x_i)^2$$\n",
    "\n",
    "This is a regular Ridge Regression (RR)!\n",
    "\n",
    "with <font color='green'> $y_i' = \\sqrt{w_i}y_i$</font> and <font color='green'>$x_i' = \\sqrt{w_i}x_i$ </font>\n",
    "$$$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In matrix form:\n",
    "\n",
    "Introducing the diagonal matrix $W = \\mathrm{diag}(w_1, ..., w_n)$, we can rewrite the objective\n",
    "$$\\sum_{i=1}^n w_i (y_i - \\beta^\\top x_i)^2 = (Y - X\\beta)^\\top W (Y - X\\beta)$$\n",
    "\n",
    "We now write $W = W^\\frac{1}{2}W^\\frac{1}{2} = (W^\\frac{1}{2})^\\top W^\\frac{1}{2}$, where $W^\\frac{1}{2} = \\mathrm{diag}(\\sqrt{w_1}, ..., \\sqrt{w_n})$\n",
    "$$$$\n",
    "\n",
    "The objective becomes:\n",
    "$$\n",
    "\\frac{1}{n} (W^\\frac{1}{2}Y - W^\\frac{1}{2}X\\beta)^\\top  (W^\\frac{1}{2}Y - W^\\frac{1}{2}X\\beta) + \\lambda \\|\\beta\\|^2 = \\frac{1}{n} \\|Y' - X'\\beta\\|^2 + \\lambda \\|\\beta\\|^2 \\,\n",
    "$$\n",
    "with <font color='green'>$Y' = W^\\frac{1}{2} Y$</font> and <font color='green'>$X' = W^\\frac{1}{2} X$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weighted Ridge Regression (WRR)\n",
    "def solveWRR(y, X, w, lam):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == len(w) == n)\n",
    "    \n",
    "    y1 = np.sqrt(w) * y\n",
    "    X1 = (np.sqrt(w) * X.T).T\n",
    "    # Hint:\n",
    "    # Find y1 and X1 such that:\n",
    "    beta = solveRR(y1, X1, lam)\n",
    "    return (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it out:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our solver:\n",
      "[ 1.21907902  0.69464471  0.05119819 -0.50155617  0.60130538  0.2718474\n",
      "  0.6571524   0.60033163  0.862139   -0.50913719]\n",
      "Scikit-learn:\n",
      "[ 1.21907902  0.69464471  0.05119819 -0.50155617  0.60130538  0.2718474\n",
      "  0.6571524   0.60033163  0.862139   -0.50913719]\n",
      "\n",
      "Difference between the two:\n",
      "4.93519548249e-32\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "lam = 0.1\n",
    "w = np.random.rand(len(y))\n",
    "\n",
    "# Our solver\n",
    "beta1 = solveWRR(y, X, w, lam)\n",
    "\n",
    "# Python solver\n",
    "alpha = lam * X.shape[0]\n",
    "model = lm.Ridge(alpha=alpha, fit_intercept=False, normalize=False)\n",
    "beta2 = model.fit(X, y, sample_weight=w).coef_\n",
    "\n",
    "# Check\n",
    "compare(beta1, beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Logistic Ridge Regression (LRR)\n",
    "\n",
    "Given $X \\in \\mathbb{R}^{n \\times p}$ and $y \\in \\{-1,+1\\}^n$, solve\n",
    "$$\n",
    "\\min_{\\beta \\in \\mathbb{R}^p} \\frac{1}{n} \\sum_{i=1}^n \\log (1+e^{-y_i \\beta^\\top x_i}) + \\lambda \\|\\beta\\|^2 \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ be the sigmoid function.\n",
    "\n",
    "Compute $\\sigma'(x)$\n",
    "\n",
    "$$\\sigma'(x) = \\frac{e^{-x}}{(1+e^{-x})^2} = \\frac{1}{1+e^{-x}} - \\frac{1}{(1+e^{-x})^2} = \\sigma(x)(1-\\sigma(x)) = \\sigma(x)\\sigma(-x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Under the logistic model, <font color='green'>$\\sigma(y_i\\beta^\\top x_i) = \\mathbb{P}[y=y_i \\mid x_i, \\beta]$</font>\n",
    "\n",
    "- $\\mathbb{P}[y_i = 1 \\mid x_i, \\beta] = \\sigma(\\beta^\\top x_i)$ (definition of the model)\n",
    "- $\\mathbb{P}[y_i = 0 \\mid x_i, \\beta] = 1 - \\sigma(\\beta^\\top x_i) = \\sigma(-\\beta^\\top x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewriting $J$:\n",
    "$$\n",
    "J(\\beta) = - \\frac{1}{n} \\sum_{i=1}^n {\\log(\\sigma(y_i\\beta^\\top x_i))} + \\lambda \\|\\beta\\|^2 \\,.\n",
    "$$\n",
    "\n",
    "Compute its gradient $\\nabla J$, and its Hessian $\\nabla^2 J$\n",
    "$$$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the gradient:\n",
    "$$\n",
    "\\nabla J(\\beta) = - \\frac{1}{n} \\sum_{i=1}^n {y_i \\sigma(-y_i\\beta^\\top x_i) x_i} + 2\\lambda \\beta \\,.\n",
    "$$\n",
    "\n",
    "In matrix form, with $P(\\beta) = \\mathrm{diag}\\left[\\sigma(-y_i\\beta^\\top x_i)\\right]$:\n",
    "<font color='green'>\n",
    "$$\n",
    "\\nabla J(\\beta) = - \\frac{1}{n} X^T P(\\beta)y + 2\\lambda \\beta \\,.\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hessian matrix:\n",
    "$$\n",
    "\\nabla^2 J(\\beta) = \\nabla \\left(\\nabla J(\\beta)\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla^2 J(\\beta) = - \\frac{1}{n} \\sum_{i=1}^n {\\sigma(y_i\\beta^\\top x_i)\\sigma(-y_i\\beta^\\top x_i) x_i x_i^\\top} + 2\\lambda I \\,\n",
    "$$\n",
    "\n",
    "Define $w_i = \\sigma(y_i\\beta^\\top x_i)\\sigma(-y_i\\beta^\\top x_i)$ and $W = \\mathrm{diag}(w_1, ..., w_n)$\n",
    "$$$$\n",
    "<font color='green'>\n",
    "$$\n",
    "\\nabla^2 J(\\beta) = - \\frac{1}{n} X^\\top W X + 2\\lambda I \\,\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Note: Gradient descent:**\n",
    "$$\n",
    "\\beta^{new} \\leftarrow \\beta^{old} - h \\nabla J(\\beta^{old})\n",
    "$$\n",
    "where $h$ is the step size (learning rate)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving for optimal $\\beta$ using Newton-Raphson\n",
    "$$\n",
    "\\beta^{new} \\leftarrow \\beta^{old} - \\left(\\nabla^2 J(\\beta^{old})\\right)^{-1} \\nabla J(\\beta^{old})\n",
    "$$\n",
    "\n",
    "This algorithm converges to the optimal $\\beta$ (the one that minimizes $J$)\n",
    "$$$$\n",
    "\n",
    "$\\approx$ gradient descent with adapted step size\n",
    "\n",
    "Show that each step is equivalent to solving a weighted ridge regression (WRR). Hence we can compute $\\beta$ without inverting the Hessian.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one dimension:\n",
    "$$\n",
    "f(x) \\approx f(x_0) + f'(x_0)(x-x_0) + \\frac{1}{2}f\"(x_0)(x-x_0)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>Quadratic approximation to $J$</font>:\n",
    "\n",
    "$$\n",
    "J(\\beta) \\approx J_q(\\beta) := J(\\beta^{old}) + (\\beta - \\beta^{old})^\\top \\nabla J(\\beta^{old}) + \\frac{1}{2} (\\beta - \\beta^{old})^\\top \\nabla^2 J(\\beta^{old}) (\\beta - \\beta^{old})\n",
    "$$\n",
    "\n",
    "**lemma**: $\\min_\\beta J_q(\\beta) = \\beta^{new} = \\beta^{old} - \\left(\\nabla^2 J(\\beta^{old})\\right)^{-1} \\nabla J(\\beta^{old})$\n",
    "\n",
    "*proof*: $$\\nabla J_q(\\beta) = \\nabla J(\\beta^{old}) + \\nabla^2 J(\\beta^{old}) (\\beta - \\beta^{old})$$\n",
    "$$\\nabla J_q(\\beta) = 0 \\iff \\beta = \\beta^{old} - \\left(\\nabla^2 J(\\beta^{old})\\right)^{-1} \\nabla J(\\beta^{old})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next **show that $J_q$ is a WRR objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write all the terms in $J_q$ that depend on $\\beta$:\n",
    "$$$$\n",
    "\n",
    "- $\\beta^\\top \\nabla J(\\beta^{old}) = -\\frac{1}{n} \\beta^\\top X^\\top P(\\beta^{old}) y + 2\\lambda \\beta^\\top \\beta^{old}$\n",
    "- -$\\beta^\\top \\nabla^2 J(\\beta^{old}) \\beta^{old} = -\\frac{1}{n} \\beta^\\top X^\\top W(\\beta^{old}) X \\beta^{old} - 2\\lambda \\beta^\\top \\beta^{old}$\n",
    "- $\\frac{1}{2}\\beta^\\top \\nabla^2 J(\\beta^{old}) \\beta = \\frac{1}{2n} \\beta^\\top X^\\top W(\\beta^{old}) X \\beta + \\lambda \\beta^\\top \\beta$\n",
    "\n",
    "Putting it all together:\n",
    "$$2J_q(\\beta) = \\frac{1}{n} \\beta^\\top X^\\top W X \\beta - \\frac{2}{n} \\beta^\\top X^\\top W \\left(X \\beta^{old} + W^{-1}Py\\right) + 2\\lambda \\|\\beta\\|^2 + \\mathrm{Constant}$$\n",
    "\n",
    "We now define $z = X \\beta^{old} + W^{-1}Py$ and we can rewrite $J_q(\\beta)$ as:\n",
    "$$$$\n",
    "<font color='green'>\n",
    "$$\n",
    "2J_q(\\beta) = (z - X\\beta)^\\top W (z - X\\beta) + 2\\lambda \\|\\beta\\|^2 + \\mathrm{C}\n",
    "$$\n",
    "</font>\n",
    "\n",
    "We recognize here a **weighted ridge regression!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Recap for IRLS:\n",
    "\n",
    "- **Goal:** Minimize logistic loss\n",
    "$$\n",
    "J(\\beta) = - \\frac{1}{n} \\sum_{i=1}^n {\\log(\\sigma(y_i\\beta^\\top x_i))} + \\lambda \\|\\beta \\|^2\\,.\n",
    "$$\n",
    "- **Method:** Newton-Raphson\n",
    "\n",
    "Start with $\\beta^{old} = \\beta_0$.\n",
    "\n",
    "Then update until convergence:\n",
    "$$\n",
    "\\beta^{new} \\leftarrow \\beta^{old} - \\left(\\nabla^2 J(\\beta^{old})\\right)^{-1} \\nabla J(\\beta^{old})\n",
    "$$\n",
    "\n",
    "Or **equivalently**\n",
    "$$\n",
    "\\beta^{new} \\leftarrow \\min_\\beta J_q(\\beta)\n",
    "$$\n",
    "where \n",
    "$$\n",
    "2J_q(\\beta) = (z - X\\beta)^\\top W (z - X\\beta) + 2\\lambda \\|\\beta\\|^2 + \\mathrm{C}\n",
    "$$\n",
    "Each step of the Newton-Raphson, we solve a weighted Ridge regression!\n",
    "\n",
    "\n",
    "Notations used:\n",
    "- $P = \\mathrm{diag}\\left[\\sigma(-y_i\\beta^\\top x_i)\\right]$\n",
    "- $W = \\mathrm{diag}(w_1, ..., w_n)$ with $w_i = \\sigma(y_i\\beta^\\top x_i)\\sigma(-y_i\\beta^\\top x_i)$\n",
    "- $z = X \\beta^{old} + W^{-1}Py$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Recap for IRLS (code):\n",
    "\n",
    "- Set:\n",
    "    - $f_i = (\\beta^{old})^\\top x_i$\n",
    "    - $W = \\mathrm{diag}\\left[\\sigma(f_i)\\sigma(-f_i)\\right] = \\mathrm{diag}\\left[\\sigma(f_1)\\sigma(-f_1), ..., \\sigma(f_n)\\sigma(-f_n)\\right]$\n",
    "    - $z_i = f_i + \\frac{y_i} {\\sigma(y_i f_i)}$\n",
    "\n",
    "    *(You can check that $\\frac{-\\sigma(y_i f_i)}{\\sigma(f_i)\\sigma(-f_i)} = \\frac{1} {\\sigma(y_i f_i)}$*)\n",
    "\n",
    "- Call `solveWRR(z, W, X, 2`$\\lambda$`)` at each update\n",
    "\n",
    "- Stop when some criterion is reached, for example:\n",
    "    - max number of iterations is reached\n",
    "    - difference between 2 iterations is less than a threshold $\\epsilon$: \n",
    "$$\\|\\beta^{new}-\\beta^{old}\\| < \\epsilon$$\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Ridge Regression (LRR)\n",
    "def solveLRR(y, X, lam):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == n)\n",
    "    \n",
    "    # Parameters\n",
    "    max_iter = 500\n",
    "    eps = 1e-3\n",
    "    sigmoid = lambda a: 1/(1 + np.exp(-a))\n",
    "    \n",
    "    # Initialize\n",
    "    beta = np.zeros(p)\n",
    "            \n",
    "    # Hint: Use IRLS\n",
    "    for i in range(max_iter):\n",
    "        beta_old = beta\n",
    "        f = X.dot(beta_old)\n",
    "        w = sigmoid(f) * sigmoid(-f)\n",
    "        z = f + y / sigmoid(y*f)\n",
    "        beta = solveWRR(z, X, w, 2*lam)\n",
    "        # Break condition (achieved convergence)\n",
    "        #if np.sum((beta-beta_old)**2) < eps:\n",
    "        #    break\n",
    "    return (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it out:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our solver:\n",
      "[ 0.49767153  0.31536881 -0.08695287 -0.16928258  0.27290435  0.15201944\n",
      "  0.35505565  0.31972339  0.30170849 -0.39445252]\n",
      "Scikit-learn:\n",
      "[[ 0.4976808   0.31535489 -0.08695986 -0.16926035  0.27290697  0.15200484\n",
      "   0.35506729  0.31972341  0.30172333 -0.39445142]]\n",
      "\n",
      "Difference between the two:\n",
      "1.40004809416e-09\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "y_bin = np.sign(y) # Binarize targets\n",
    "lam = 0.1\n",
    "\n",
    "# Our solver\n",
    "beta1 = solveLRR(y_bin, X, lam)\n",
    "\n",
    "# Python solver\n",
    "alpha = 2 * lam * X.shape[0]\n",
    "model = lm.LogisticRegression(C=1/alpha, fit_intercept=False)\n",
    "beta2 = model.fit(X, y_bin).coef_\n",
    "\n",
    "# Check\n",
    "compare(beta1, beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Mini Data Challenge\n",
    "\n",
    "We will try to predict whether patients have breast cancer.\n",
    "\n",
    "We use scikit-learn's [breast cancer dataset](https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset)\n",
    "\n",
    "30 features, 569 samples, 2 labels ('malignant' or 'benign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data and split into training / validation sets\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "# Scaling is important\n",
    "X = sklearn.preprocessing.scale(X)\n",
    "# X = (X - X.mean()) / X.std()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit our model and compute its parameters\n",
    "lam = 0.01\n",
    "beta = solveLRR(y_train, X_train, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute predicted probabilities and classes\n",
    "probas_pred = 1/(1+np.exp(-beta.T.dot(X_test.T)))\n",
    "y_pred = np.round(probas_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model's performance:\n",
      "Accuracy: 97.87%\n",
      "AUC: 99.75%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "print(\"Our model's performance:\")\n",
    "print('Accuracy: {:.2%}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('AUC: {:.2%}'.format(roc_auc_score(y_test, probas_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.97        67\n",
      "          1       0.99      0.98      0.98       121\n",
      "\n",
      "avg / total       0.98      0.98      0.98       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daea4d83dc914cb3b4448bada32ff93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(axes=[Axis(orientation='vertical', scale=LinearScale()), Axis(scale=LinearScale())], fig…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bqplot.pyplot as plt\n",
    "\n",
    "plt.figure(title='Feature importances')\n",
    "plt.bar(np.arange(len(beta)), beta)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
