{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"font-family:verdana;font-size:300%;text-align:center;background-color:#f2f2f2;color:#0d0d0d\">AMMI NLP - Review sessions</h1>\n",
    "\n",
    "<h1 style=\"font-family:verdana;font-size:180%;text-align:Center;color:#993333\"> Lab 1: Introduction to wordvectors </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(filename):\n",
    "    fin = io.open(filename, 'r', encoding='utf-8', newline='\\n')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.asarray([float(x) for x in tokens[1:]])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ** Word vectors ** \n",
      "\n",
      "<class 'numpy.ndarray'> 300\n"
     ]
    }
   ],
   "source": [
    "# Loading word vectors\n",
    "\n",
    "print('')\n",
    "print(' ** Word vectors ** ')\n",
    "print('')\n",
    "\n",
    "'''\n",
    "word_vectors is a dictionary that maps words to their numerical word vector\n",
    "[word (string)] = [np-array] \n",
    "'''\n",
    "word_vectors = load_vectors('wiki.en.vec')\n",
    "\n",
    "tree_vector = word_vectors['tree']\n",
    "print(type(tree_vector), len(tree_vector))\n",
    "#tree_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function computes the cosine similarity between vectors u and v\n",
    "\n",
    "def cosine(u, v):\n",
    "    \n",
    "    '''\n",
    "    Parameters:\n",
    "    u : 1-D numpy array\n",
    "    v : 1-D numpy array \n",
    "    \n",
    "    Returns:\n",
    "    cos (float) : value of the cosine similairy between vectors u, v \n",
    "    '''\n",
    "\n",
    "    ## FILL CODE\n",
    "    \n",
    "    cos = np.dot(u, v)/(np.linalg.norm(u) * np.linalg.norm(v))\n",
    "    \n",
    "    return cos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity(apple, apples) = 0.637\n",
      "similarity(apple, banana) = 0.431\n",
      "similarity(apple, tiger) = 0.212\n"
     ]
    }
   ],
   "source": [
    "# compute similarity between words\n",
    "\n",
    "print('similarity(apple, apples) = %.3f' %\n",
    "      cosine(word_vectors['apple'], word_vectors['apples']))\n",
    "print('similarity(apple, banana) = %.3f' %\n",
    "      cosine(word_vectors['apple'], word_vectors['banana']))\n",
    "print('similarity(apple, tiger) = %.3f' %\n",
    "      cosine(word_vectors['apple'], word_vectors['tiger']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for nearest neighbor\n",
    "## This function returns the word corresponding to \n",
    "## nearest neighbor vector of x\n",
    "## The list exclude_words can be used to exclude some\n",
    "## words from the nearest neighbors search\n",
    "\n",
    "def nearest_neighbor(x, word_vectors, exclude_words=[]):\n",
    "    '''\n",
    "    Parameters:\n",
    "    x (string): word to find its nearest neighbour \n",
    "    word_vectors (Python dict): {word (string): np-array of word vector}\n",
    "    exclude_words (list of strings): words to be excluded from the search\n",
    "    \n",
    "    Returns:\n",
    "    best_word (string) : the word whose word vector is the nearest neighbour \n",
    "    to the word vector of x\n",
    "    '''\n",
    "    \n",
    "    best_score = -1.0\n",
    "    best_word = None\n",
    "\n",
    "    ## FILL CODE\n",
    "    \n",
    "    for word, v in word_vectors.items():\n",
    "        score = cosine(x, v)\n",
    "        if score > best_score and not word in exclude_words:\n",
    "            best_score = score\n",
    "            best_word = word \n",
    "            \n",
    "    return best_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The nearest neighbor of cat is: dog\n"
     ]
    }
   ],
   "source": [
    "print('')\n",
    "print('The nearest neighbor of cat is: ' +\n",
    "      nearest_neighbor(word_vectors['cat'], word_vectors, exclude_words = ['cat', 'cats']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint (using python priorty queues with the heapq datastructure): \n",
    "if you don't want to store all the words and scores you can use the priortiy queue and only store the best K element so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## This function return the words corresponding to the\n",
    "## K nearest neighbors of vector x.\n",
    "## You can use the functions heappush and heappop.\n",
    "\n",
    "def knn(x, vectors, k):\n",
    "    '''\n",
    "    Parameters:\n",
    "    x (string): word to find its nearest neighbour \n",
    "    word_vectors (Python dict): {word (string): np-array of word vector}\n",
    "    k (int): number of nearest neighbours to be found\n",
    "    \n",
    "    Returns: \n",
    "    k_nearest_neighbors (list of tuples): [(score, word), (score, word), ....]\n",
    "    '''\n",
    "\n",
    "    k_nearest_neighbors = None\n",
    "    ## FILL CODE\n",
    "    word_scores  =[]\n",
    "    \n",
    "    for word, v in vectors.items():\n",
    "        score = cosine(x, v)\n",
    "        word_scores.append((score, word))\n",
    "        \n",
    "    word_scores.sort(key = lambda x: x[0], reverse = True)\n",
    "    k_nearest_neighbors = word_scores[1:k+1]\n",
    "        \n",
    "    return k_nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cat\n",
      "--------------\n",
      "cats\t0.732\n",
      "dog\t0.638\n",
      "pet\t0.573\n",
      "rabbit\t0.549\n",
      "dogs\t0.538\n"
     ]
    }
   ],
   "source": [
    "knn_cat = knn(word_vectors['cat'], word_vectors, 5)\n",
    "print('')\n",
    "print('cat')\n",
    "print('--------------')\n",
    "for score, word in knn(word_vectors['cat'], word_vectors, 5):\n",
    "    print (word + '\\t%.3f' % score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: \n",
    "To find the analogies, we find the nearest neighbour associated with the wordvector d\n",
    "$$ d = \\frac{c}{\\Vert {c} \\Vert} + \\frac{b}{\\Vert {b} \\Vert} - \\frac{a}{\\Vert {a} \\Vert}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function return the words d, such that a:b and c:d\n",
    "## verifies the same relation\n",
    "\n",
    "def analogy(a, b, c, word_vectors):\n",
    "    '''\n",
    "    Parameters:\n",
    "    a (string): word a\n",
    "    b (string): word b\n",
    "    c (string): word c\n",
    "    word_vectors (Python dict): {word (string): np-array of word vector}\n",
    "    \n",
    "    Returnrs: \n",
    "    the word d (string) associated with c such that c:d is similar to a:b \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ## FILL CODE\n",
    "    \n",
    "    va = word_vectors[a]\n",
    "    vb = word_vectors[b]\n",
    "    vc = word_vectors[c]\n",
    "    \n",
    "    va = va/np.linalg.norm(va)\n",
    "    vb = vb/np.linalg.norm(vb)\n",
    "    vc = vc/np.linalg.norm(vc)\n",
    "    \n",
    "    return nearest_neighbor(vc + vb - va, word_vectors, [a, b, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "france - paris + rome = italy\n"
     ]
    }
   ],
   "source": [
    "# Word analogies\n",
    "\n",
    "print('')\n",
    "print('france - paris + rome = ' + analogy('paris', 'france', 'rome', word_vectors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "similarity(genius, man) = 0.445\n",
      "similarity(genius, woman) = 0.325\n"
     ]
    }
   ],
   "source": [
    "## A word about biases in word vectors:\n",
    "\n",
    "print('')\n",
    "print('similarity(genius, man) = %.3f' %\n",
    "      cosine(word_vectors['man'], word_vectors['genius']))\n",
    "print('similarity(genius, woman) = %.3f' %\n",
    "      cosine(word_vectors['woman'], word_vectors['genius']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the association strength between:\n",
    "##   - a word w\n",
    "##   - two sets of attributes A and B\n",
    "\n",
    "def association_strength(w, A, B, vectors):\n",
    "    '''\n",
    "    Parameters:\n",
    "    w (string): word w\n",
    "    A (list of strings): The words belonging to set A\n",
    "    B (list of strings): The words belonging to set B\n",
    "    vectors (Python dict): {word (string): np-array of word vector}\n",
    "    \n",
    "    Returnrs: \n",
    "    strength (float): the value of the association strength \n",
    "    '''\n",
    "    \n",
    "    part_a = 0.0\n",
    "    part_b = 0.0 \n",
    "    ## FILL CODE\n",
    "    for a in A:\n",
    "        part_a += cosine(vectors[w], vectors[a])\n",
    "    part_a/=len(A) \n",
    "    \n",
    "    for b in B:\n",
    "        part_b += cosine(vectors[w], vectors[b])\n",
    "    part_b/=len(B)\n",
    "    \n",
    "    strength = part_a - part_b\n",
    "    return strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform the word embedding association test between:\n",
    "##   - two sets of words X and Y\n",
    "##   - two sets of attributes A and B\n",
    "\n",
    "def weat(X, Y, A, B, vectors):\n",
    "    '''\n",
    "    Parameters:\n",
    "    X (list of strings): The words belonging to set X\n",
    "    Y (list of strings): The words belonging to set Y\n",
    "    A (list of strings): The words belonging to set A\n",
    "    B (list of strings): The words belonging to set B\n",
    "    vectors (Python dict): {word (string): np-array of word vector}\n",
    "    \n",
    "    Returnrs: \n",
    "    score (float): the value of the group association strength  \n",
    "    '''\n",
    "    \n",
    "    score = 0.0\n",
    "    ## FILL CODE\n",
    "    part_a = 0.0\n",
    "    part_b = 0.0 \n",
    "    \n",
    "    for x in X:\n",
    "        part_a += association_strength(x, A, B, vectors)\n",
    "    for y in Y:\n",
    "        part_b += association_strength(y, A, B, vectors)\n",
    "        \n",
    "    score = part_a - part_b\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word embedding association test: 0.930\n"
     ]
    }
   ],
   "source": [
    "## Replicate one of the experiments from:\n",
    "##\n",
    "## Semantics derived automatically from language corpora contain human-like biases\n",
    "## Caliskan, Bryson, Narayanan (2017)\n",
    "\n",
    "career = ['executive', 'management', 'professional', 'corporation', \n",
    "          'salary', 'office', 'business', 'career']\n",
    "\n",
    "family = ['home', 'parents', 'children', 'family',\n",
    "          'cousins', 'marriage', 'wedding', 'relatives']\n",
    "\n",
    "male = ['john', 'paul', 'mike', 'kevin', 'steve', 'greg', 'jeff', 'bill']\n",
    "female = ['amy', 'joan', 'lisa', 'sarah', 'diana', 'kate', 'ann', 'donna']\n",
    "\n",
    "print('')\n",
    "print('Word embedding association test: %.3f' %\n",
    "      weat(career, family, male, female, word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
